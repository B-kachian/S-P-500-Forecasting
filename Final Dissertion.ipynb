{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79aa440-3363-4f92-939b-528ed6371ef5",
   "metadata": {},
   "source": [
    "# Project title\n",
    "## Optimizing Stock Market Predictions: A Comparative Study of LSTM Networks and Time Series Models for S&P 500 Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80169bbe-6256-4b76-ad66-a6acc9080836",
   "metadata": {},
   "source": [
    "### BEHNAM KACHIAN\n",
    "### Supervisor: Dr. Raju Chinthalapati\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2445ace-a7ed-48b5-97ea-abdda5def376",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This thesis investigates the prediction of stock market prices by leveraging advanced machine learning techniques, with a specific focus on forecasting the closing prices of stocks within the S&P 500. The rise of machine learning, coupled with the availability of extensive datasets, has greatly enhanced the accuracy of financial forecasting, offering powerful tools for investors, financial analysts, and policymakers. The primary objective of this research is to develop and evaluate models that can significantly improve the precision and reliability of stock market predictions.\r\n",
    "The project centres on constructing and comparing various time series and regression models to identify the most effective method for predicting stock closing prices. Special attention is given to Long Short-Term Memory (LSTM) networks, known for their ability to capture sequential dependencies in time series data. Additionally, models such as ARIMA, SARIMA, Prophet, and traditional regression models are explored. The study’s objectives include:\r\n",
    "1.\tDeveloping a robust model to predict the closing prices of stocks in the S&P 500.\r\n",
    "2.\tEvaluating the performance of time series and regression models to determine the most accurate predictor.\r\n",
    "3.\tAnalyzing stock market data over a 24-year period to understand the factors influencing price movements.\r\n",
    "To achieve these objectives, the project involves collecting a comprehensive dataset of historical stock prices, applying feature engineering techniques to enhance model performance, and experimenting with various model parameters to optimize prediction accuracy. The methodology includes the application of cross-validation techniques, hyperparameter tuning, and performance evaluation using metrics such as RMSE, MAPE, and R-squared.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea4120-e275-421c-a06a-afff1a6a11ab",
   "metadata": {},
   "source": [
    "## Explanation of Dataset\n",
    "In this extensive financial analysis project, meticulous integration of seven diverse datasets has been achieved to create a holistic view of the financial and economic landscapes. This integration spans various dimensions of market behaviour, economic indicators, and individual stock performance, covering a period from January 1, 2001, to June 1, 2024. The objective is to harness a wide array of data points to provide a multifaceted analysis that supports predictive modelling, investment strategy formulation, and economic forecasting.\n",
    "   The Combined Datasets:\n",
    "•\tS&P 500 U.S. Stock Market Dataset\n",
    "•\tCBOE Volatility Index (VIX) Dataset\n",
    "•\tEffective Federal Funds Rate (EFFR) Dataset\n",
    "•\tU.S. Dollar Index (USDX) Dataset\n",
    "•\tU.S. Unemployment Rate (UNRATE) Dataset\n",
    "•\tUniversity of Michigan Consumer Sentiment Index (UMCSENT) Dataset.\n",
    "•\tApple Inc. (AAPL) Stock Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aab229-0eae-40dd-8d99-dea296b6b43a",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "Running the code may require a significant amount of time because of its computational demands, particularly during model tuning. Utilizing a GPU is recommended to speed up the process. The dataset used, sourced from Yahoo Finance, can be easily fetched during code execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955f490-cb82-4a23-8e07-5ed82fc6f95f",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "### Importing Necessary Libraries and Installing Packages\n",
    "### This section handles the installation of libraries required for the project, ensuring that all necessary Python packages are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e3fb1-97fe-48bc-8b3c-4ba3bb207ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce7e3b-8349-4bb6-aa30-8e223a747051",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a743c9-cbae-4a6d-af4d-4af22284fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b85067-d611-4dbc-bdec-e994d592cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c52c6-27b1-4d28-a831-5feb02793e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c02e2e-287f-4250-b018-918c28417abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade shap tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc3951-591f-45d8-bf46-d443dcecbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a53b0-54b2-4d14-a5e6-1b94f2159d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717a163-f1c5-4260-aecb-6c8395eaaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy statsmodels matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca37659-b4a9-476e-8304-254707a8bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: System and Version Checks\n",
    "import sys  # For system-specific parameters and functions\n",
    "assert sys.version_info >= (3, 5), \"Python ≥3.5 is required\"  # Ensure Python version is at least 3.5\n",
    "import sklearn  # For checking scikit-learn version\n",
    "assert sklearn.__version__ >= \"1.0\", \"Scikit-Learn ≥1.0 is required\"  # Ensure scikit-learn version is at least 1.0\n",
    "\n",
    "\n",
    "# Section 2: Core Data Handling and Visualization Libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and array manipulations\n",
    "import matplotlib.pyplot as plt  # For static and interactive visualizations\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "import matplotlib as mpl  # For customizing Matplotlib settings\n",
    "import datetime  # For date and time operations\n",
    "\n",
    "# Configure Matplotlib settings\n",
    "#mpl.rc('axes', labelsize=14)  # Set label size for axes\n",
    "#mpl.rc('xtick', labelsize=12)  # Set label size for x-axis ticks\n",
    "#mpl.rc('ytick', labelsize=12)  # Set label size for y-axis ticks\n",
    "#plt.style.use('ggplot')  # Use 'ggplot' style for Matplotlib plots\n",
    "\n",
    "\n",
    "# Section 3: Advanced Data Visualization Tools\n",
    "import plotly.express as px  # For creating interactive visualizations with Plotly\n",
    "import plotly.graph_objects as go  # For complex visualizations with Plotly\n",
    "from plotly.subplots import make_subplots  # For creating subplot grids in Plotly\n",
    "import plotly.figure_factory as ff  # For advanced visualizations with Plotly\n",
    "\n",
    "\n",
    "# Section 4: Machine Learning and Time Series Analysis Tools\n",
    "import xgboost as xgb  # For gradient boosting models\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, KFold, GridSearchCV  # For splitting data and hyperparameter tuning\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error  # For model evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # For feature scaling and normalization\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor  # Ensemble methods for regression\n",
    "from sklearn.tree import DecisionTreeRegressor  # Decision tree algorithm for regression\n",
    "from sklearn.neighbors import KNeighborsRegressor  # k-Nearest Neighbors algorithm for regression\n",
    "from sklearn.linear_model import LinearRegression  # Linear regression model\n",
    "from statsmodels.tsa.stattools import adfuller  # Augmented Dickey-Fuller test for stationarity\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf  # Autocorrelation and Partial Autocorrelation plots\n",
    "from statsmodels.tsa.arima.model import ARIMA  # ARIMA model for time series forecasting\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX  # SARIMAX model for time series forecasting with seasonality\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL  # Seasonal decomposition of time series\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR  # Vector Autoregression model for multivariate time series\n",
    "from pandas_datareader import data as pdr  # For fetching financial data\n",
    "\n",
    "\n",
    "# Section 5: Deep Learning Tools\n",
    "from tensorflow import keras  # For deep learning with Keras\n",
    "from tensorflow.keras.models import Sequential, clone_model  # For building and cloning neural network models\n",
    "from tensorflow.keras.layers import LSTM, Dense, Attention, TimeDistributed, Bidirectional, Dropout, InputLayer  # Layers for deep learning models\n",
    "from tensorflow.keras.optimizers import Adagrad, Nadam, Adam, SGD  # Optimization algorithms\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # To stop training when a monitored metric stops improving\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError  # Metric for model evaluation\n",
    "from tensorflow.keras.regularizers import L1L2  # Regularization techniques to prevent overfitting\n",
    "from tensorflow.keras import backend as K  # Backend functions for Keras\n",
    "\n",
    "\n",
    "# Section 6: Finance-specific Libraries\n",
    "import yfinance as yf  # For retrieving financial data from Yahoo Finance\n",
    "import datetime  # For date and time operations (already imported in Section 2)\n",
    "\n",
    "\n",
    "# Section 7: Model Interpretability\n",
    "import shap  # SHAP library for model interpretability and feature importance\n",
    "\n",
    "# Section 8: Configuration and Warning Management\n",
    "import warnings  # For managing warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)  # Ignore deprecation warnings specifically\n",
    "warnings.filterwarnings('ignore')  # Ignore all other warnings\n",
    "pd.options.plotting.backend = \"plotly\"  # Set default plotting backend to 'plotly' for enhanced visualizations\n",
    "\n",
    "# Ensure that plots are displayed inline in Jupyter Notebook environments\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfec09-7378-44cd-b7c8-f7dd4d92b7e4",
   "metadata": {},
   "source": [
    "#### Improvements Made:\n",
    "1.Consolidated Import Statements: All necessary libraries are imported once, without redundancy.\n",
    "\n",
    "2.Structured and Organized Sections: Each section is specifically labeled and dedicated to different aspects such as system checks, data handling, visualization, machine learning, and deep learning tools.\n",
    "\n",
    "3.Configuration Management: Matplotlib styles and warning configurations are set up once, reflecting the need for consistency across the project.\n",
    "\n",
    "4.Inline Plotting: Ensures that %matplotlib inline is declared once to apply to all Matplotlib plots within a Jupyter notebook.\n",
    "\n",
    "5.Version and Dependency Checks: Ensures the project's Python environment and library dependencies are correctly set up before execution, preventing runtime errors due to version mismatches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55011755-ac48-4903-b2be-27a24d47234d",
   "metadata": {},
   "source": [
    "# Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72952e-b5fb-450a-b221-6dd120d211f2",
   "metadata": {},
   "source": [
    "## Loading S&P 500, US stock market Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2321a9-6bce-443a-8178-512866e02c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Assuming you are fetching the data using yfinance\n",
    "ticker_symbol = \"^GSPC\"  # S&P 500\n",
    "data = yf.download(ticker_symbol, start=\"2001-01-01\", end=\"2024-06-30\")\n",
    "\n",
    "# Reset the index to make the Date part of the DataFrame\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Rename the new column to 'Date' if it's not automatically named\n",
    "data.rename(columns={'Date': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637063d-0e1e-4ef2-9a2f-86de9f0ed655",
   "metadata": {},
   "source": [
    "### Adding Average price column and Percentage Markup to Prices columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a74be-2c1b-41d6-9589-84349fba02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average price for each row by taking the mean across the 'Open', 'High', 'Low', and 'Close' columns\n",
    "data['AVG_price'] = data[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
    "\n",
    "# This represents the intraday price change from opening to closing\n",
    "data['dif_price_CO'] = data['Close'] - data['Open']\n",
    "\n",
    "# This can indicate how much the price fell from the daily high to the closing price\n",
    "data['dif_price_CH'] = data['Close'] - data['High']\n",
    "\n",
    "# This shows the rate of change in closing price day-by-day\n",
    "data['Price_Change'] = data['Close'].pct_change()\n",
    "\n",
    "# This indicates how trading volume changes day-by-day, which can reflect changes in market activity\n",
    "data['Volume_Change'] = data['Volume'].pct_change()\n",
    "\n",
    "# This provides insight into the average price movement across the session compared to the previous session\n",
    "data['Price_AVG_Change'] = data['AVG_price'].pct_change()\n",
    "\n",
    "# Output the updated DataFrame structure including data types and non-null counts for each column\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e960205-09f2-4e5f-aea3-5bd87a32bffa",
   "metadata": {},
   "source": [
    "### Clear S&P 500, US stock market Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217d1ed-33a3-4015-9463-afcf59e9dfb9",
   "metadata": {},
   "source": [
    "#### Mising Value Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fea832-14a0-42d7-8c1d-d14f3f90f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values \n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd1d88-4324-4b21-9f83-886ff7a829fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of each column and fill missing values with these means\n",
    "means = data.mean()\n",
    "data.fillna(means, inplace=True)\n",
    "# Applying the means to fill missing values directly\n",
    "data = data.apply(lambda x: x.fillna(x.mean()))\n",
    "# Check again for any missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e59dc2-50fa-4b42-b99a-6f5ca599a051",
   "metadata": {},
   "source": [
    "## S&P 500 Data SET = data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2ef78-140c-4d79-a4b2-ef0d8933884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823f3bf-3b6b-4d15-9ff1-a5c2c53f09ed",
   "metadata": {},
   "source": [
    "## Loading Chicago Board Options Exchange (CBOE) DataSet\n",
    "#### Cboe Volatility Index (VIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebec84-5670-45f7-8911-62bd76da1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ticker symbol for the CBOE Volatility Index\n",
    "vix_ticker = \"^VIX\"\n",
    "\n",
    "# Fetch historical data for the VIX\n",
    "vix_data = yf.download(vix_ticker, start=\"2001-01-01\", end=\"2024-06-30\")\n",
    "\n",
    "# Reset the index to make the Date a column\n",
    "vix_data.reset_index(inplace=True)\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "#print(vix_data.tail())\n",
    "\n",
    "print(vix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44fd33-0f56-4ca7-9659-eb9101cd790e",
   "metadata": {},
   "source": [
    "###  Clear Cboe Volatility Index (VIX) DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c4197-5d86-4367-ba62-488be83e4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values \n",
    "print(\"Missing values in each column:\")\n",
    "print(vix_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673589de-2045-44d5-91ba-1dc136fc3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'Volume' column from the vix_data DataFrame\n",
    "vix_data.drop(columns='Volume', inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "vix_data.rename(columns={\n",
    "   'Date': 'date_VIX',\n",
    "   'Open': 'open_VIX',\n",
    "   'High': 'high_VIX',\n",
    "   'Low': 'low_VIX',\n",
    "   'Close': 'close_VIX',\n",
    "   'Adj Close': 'adj_close_VIX',\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f9a46-eeed-4ff9-958b-25d584684a86",
   "metadata": {},
   "source": [
    "### Cboe Volatility Index (VIX) DataSet = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec8008-d168-4d01-9c04-6885e22eaada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = vix_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9877ea-5375-4c22-81fd-6c0915a0c969",
   "metadata": {},
   "source": [
    "## Loading Interest Rate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3a418-58d9-497a-bc79-7d130c793e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import datetime\n",
    "\n",
    "# Define the ticker for the Effective Federal Funds Rate from FRED\n",
    "effr_ticker = 'EFFR'\n",
    "\n",
    "# Define the date range using datetime objects for clarity and consistency\n",
    "start_date = datetime.datetime(2001, 1, 1)\n",
    "end_date = datetime.datetime(2024, 6, 30)\n",
    "\n",
    "# Fetch the data from FRED\n",
    "effr_data = pdr.get_data_fred(effr_ticker, start_date, end_date)\n",
    "\n",
    "# Reset the index to make 'DATE' a column, aligning with the standard DataFrame format used in your first code example\n",
    "effr_data.reset_index(inplace=True)\n",
    "\n",
    "effr_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78257035-59fa-4a8f-be0c-042bdad34110",
   "metadata": {},
   "source": [
    "## Clear Interest Rate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e0ab1-a3bd-42f8-b86f-17b591ff3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing value\n",
    "print(\"Missing values in each column:\")\n",
    "print(effr_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ba127-ad58-41aa-93f5-715ddafdd707",
   "metadata": {},
   "source": [
    "##### Interest Rate Data Set = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37adad9-b59c-4f4f-8d41-a467108c55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=effr_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22912021-2bd7-42dd-b872-fc1fb99ffdaf",
   "metadata": {},
   "source": [
    "## Loading US dollar index Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2b2f2-939a-47eb-9021-4c0bec1e4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Define the ticker symbol for the U.S. Dollar Index\n",
    "usdx_ticker = \"DX=F\"\n",
    "\n",
    "# Fetch historical data for the U.S. Dollar Index\n",
    "usdx_data = yf.download(usdx_ticker, start=\"2001-01-01\", end=\"2024-06-30\")\n",
    "\n",
    "# Reset the index to make the Date part of the DataFrame\n",
    "usdx_data.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "usdx_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41334b2f-6d6f-473f-943a-e3f4985daa2a",
   "metadata": {},
   "source": [
    "### Clear US dollar index Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9aea4f-afe5-4bd7-9372-8031de8247fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing value\n",
    "print(\"Missing values in each column:\")\n",
    "print(usdx_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933345e6-26c6-4077-a11c-d75b701288c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "usdx_data.rename(columns={\n",
    "    'Date' :'Date_USDX',\n",
    "    'Open': 'open_USDX',\n",
    "    'High': 'high_USDX',\n",
    "    'Low': 'low_USDX',\n",
    "    'Close': 'close_USDX',\n",
    "    'Adj Close': 'Adj_USDX',\n",
    "    'Volume': 'Volume_USDX'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame to verify the changes\n",
    "print(usdx_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b59ec1-9194-434e-aee8-0c38f77ef527",
   "metadata": {},
   "source": [
    "## US dollar index Data set = data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f6acc-af32-4f35-be1e-b625cd0bd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = usdx_data\n",
    "data4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb33bfd-6171-4aea-83c9-6e2bb849ccf3",
   "metadata": {},
   "source": [
    "## Loading Civilian unemployment rate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f885c2-2372-4898-8c5e-3736a6daa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import datetime\n",
    "\n",
    "# Define the date range using datetime objects for clarity and consistency\n",
    "start_date = datetime.datetime(2001, 1, 1)\n",
    "end_date = datetime.datetime(2024, 6, 30)\n",
    "\n",
    "# Fetch the data\n",
    "unrate_data = pdr.get_data_fred('UNRATE', start_date, end_date)\n",
    "\n",
    "# Reset the index to make 'DATE' a column, aligning with the standard DataFrame format used in your second code example\n",
    "unrate_data.reset_index(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be7724-db2a-438c-a2b4-211c0c0eaef1",
   "metadata": {},
   "source": [
    "### Clear Civilian unemployment rate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645339b-4dc4-46af-aea8-80df59aee10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of missing values per column\n",
    "print(\"Missing values in each column of data3:\")\n",
    "print(unrate_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a24b55-3a43-49a3-b68b-df436b989081",
   "metadata": {},
   "source": [
    "### Civilian unemployment rate Data Set = data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fc416-b4cb-489b-9a9f-f65f55a8cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5=unrate_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f6119-1357-461c-839a-29b47e2cef23",
   "metadata": {},
   "source": [
    "## Loading Consumer sentiment index Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487df3b-525d-4c56-802e-012e2e2a5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import datetime\n",
    "\n",
    "# Define the date range using datetime objects for consistency\n",
    "start_date = datetime.datetime(2001, 1, 1)\n",
    "end_date = datetime.datetime(2024, 6, 30)\n",
    "\n",
    "# Fetch the data\n",
    "umsent_data = pdr.get_data_fred('UMCSENT', start_date, end_date)\n",
    "\n",
    "# Reset the index to make 'DATE' a column, making it consistent with other DataFrame manipulations\n",
    "umsent_data.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# Display the first and last few rows to confirm the data structure\n",
    "data6=umsent_data\n",
    "data6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cca0e-8f40-4b8b-bbf2-182cf10b8cd4",
   "metadata": {},
   "source": [
    "### Clear Consumer sentiment index Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facff24a-440f-424d-ab1e-9aa4f693d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing value\n",
    "print(\"Missing values in each column:\")\n",
    "print(data6.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c25b4-9c32-4931-8eaa-7972906ef550",
   "metadata": {},
   "source": [
    "## Financial Fata For Apple Inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9028676-e4f0-4d18-838e-f27b8ca4d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download historical data for Apple Inc.\n",
    "AAPL_data = yf.download('AAPL', start='2001-01-01', end='2024-06-30')\n",
    "\n",
    "# Calculate the MACD and Signal Line indicators\n",
    "# Short term exponential moving average (EMA)\n",
    "Exp1 = AAPL_data['Close'].ewm(span=12, adjust=False).mean()\n",
    "# Long term exponential moving average (EMA)\n",
    "Exp2 = AAPL_data['Close'].ewm(span=26, adjust=False).mean()\n",
    "# MACD line\n",
    "AAPL_data['MACD'] = Exp1 - Exp2\n",
    "# Signal line\n",
    "AAPL_data['Signal_Line'] = AAPL_data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Reset the index to make 'Date' a column\n",
    "AAPL_data.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "AAPL_data.rename(columns={\n",
    "    'Open': 'open_AAPL',\n",
    "    'High': 'high_AAPL',\n",
    "    'Low': 'low_AAPL',\n",
    "    'Close': 'close_AAPL',\n",
    "    'Adj Close': 'Adj_Close_APPL',\n",
    "    'Volume': 'Volume_AAPL'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame to verify the changes\n",
    "print(AAPL_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0eb09d-af1f-458f-ad9a-ce282bf35a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate True Range (TR)\n",
    "AAPL_data['H-L'] = AAPL_data['high_AAPL'] - AAPL_data['low_AAPL']\n",
    "AAPL_data['H-PC'] = abs(AAPL_data['high_AAPL'] - AAPL_data['Adj_Close_APPL'].shift(1))\n",
    "AAPL_data['L-PC'] = abs(AAPL_data['low_AAPL'] - AAPL_data['Adj_Close_APPL'].shift(1))\n",
    "AAPL_data['TR'] = AAPL_data[['H-L', 'H-PC', 'L-PC']].max(axis=1)\n",
    "\n",
    "# Calculate ATR using the correct DataFrame\n",
    "atr_period = 14  # Typical period used for ATR\n",
    "AAPL_data['ATR'] = AAPL_data['TR'].rolling(atr_period).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ffe70-8b3c-4951-9864-8eb384de4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate price changes\n",
    "AAPL_data['Delta'] = AAPL_data['Adj_Close_APPL'].diff()\n",
    "\n",
    "# Calculate gains and losses\n",
    "AAPL_data['Gain'] = AAPL_data['Delta'].where(AAPL_data['Delta'] > 0, 0)\n",
    "AAPL_data['Loss'] = AAPL_data['Delta'].where(AAPL_data['Delta'] < 0, 0)\n",
    "\n",
    "# Calculate average gain and loss\n",
    "window = 14  # Typical period for RSI\n",
    "AAPL_data['Avg Gain'] = AAPL_data['Gain'].rolling(window=window, min_periods=1).mean()\n",
    "AAPL_data['Avg Loss'] = AAPL_data['Loss'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "# Calculate RS and RSI\n",
    "AAPL_data['RS'] = AAPL_data['Avg Gain'] / AAPL_data['Avg Loss']\n",
    "AAPL_data['RSI'] = 100 - (100 / (1 + AAPL_data['RS']))\n",
    "\n",
    "data7=AAPL_data\n",
    "data7.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421b55b-cb5f-4970-a8ab-027ac3999a72",
   "metadata": {},
   "source": [
    "## Concat all data Sets = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30346a-cf92-419f-b56d-551acdaa7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assuming all DataFrames have the same index or are aligned by some key\n",
    "df = pd.concat([data1,data2,data3,data4,data5,data6,data7], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502347d5-4148-4144-89e4-86096f880181",
   "metadata": {},
   "source": [
    "# Analyze and visualization Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850918c-79ee-47f7-840c-62e2961a8822",
   "metadata": {},
   "source": [
    "### Analyze S&P 500, US stock market Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2867b7e-9d4e-4ec7-a3b5-b4cc5dfe601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display just the columns of the DataFrame\n",
    "print(data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004ce4d-a5dd-4298-bd0b-2a32b6a907da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938686f2-a2da-4f9c-a1f0-49693c22ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24b564-b931-42fa-9912-8cde4adfb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closing price history\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data['Date'], data1['Close'], label='S&P 500 Close Price')\n",
    "plt.title('S&P 500 Close Price History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price (USD)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeca54-0ddd-4f40-8e8b-ff3aa2117045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of colors for the plots\n",
    "colors = ['yellow', 'blue', 'green', 'purple', 'red', 'orange', 'cyan', 'pink', 'gray', 'brown']\n",
    "# Ensure you have enough colors for all columns, or cycle through the colors if there are more columns than colors\n",
    "\n",
    "# Iterate through each column in the DataFrame\n",
    "for index, column in enumerate(data1.columns):\n",
    "    # Calculate value counts for the current column\n",
    "    column_data = data1[column].value_counts()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    # Create a histogram for the current column\n",
    "    # Use modulo to cycle through colors if there are more columns than colors in the list\n",
    "    sns.histplot(data1[column], bins=30, kde=False, color=colors[index % len(colors)], edgecolor='black')\n",
    "\n",
    "    # Set the title and labels\n",
    "    plt.title(f'Distribution of {column}', fontsize=15)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "    # Print the value counts\n",
    "    print(column_data)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461a2d0-cfed-4b8c-9382-29dbe502f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any conversion issues\n",
    "if data1['Date'].isnull().any():\n",
    "    print(\"Warning: Some dates could not be converted and are NaT\")\n",
    "\n",
    "# Drop rows where the date could not be converted\n",
    "data1.dropna(subset=['Date'], inplace=True)\n",
    "\n",
    "# Filter data from 2008 to 2020\n",
    "filtered_data = data1[(data1['Date'] >= '2007-01-01') & (data1['Date'] <= '2010-01-01')]\n",
    "\n",
    "# Select a subset of data for plotting\n",
    "subset_data = filtered_data  # Selecting the first 200 rows of the filtered data\n",
    "\n",
    "# Set the 'Date' column as the index\n",
    "subset_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Plotting the prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(subset_data.index, subset_data['Open'], label='Open Price', color='blue')\n",
    "plt.plot(subset_data.index, subset_data['High'], label='High Price', color='green')\n",
    "plt.plot(subset_data.index, subset_data['Low'], label='Low Price', color='red')\n",
    "plt.plot(subset_data.index, subset_data['Close'], label='Close Price', color='orange')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate by 45 degrees and align right\n",
    "plt.title('Price Comparison Over Time (207-01-01, 2010-01-01)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94097255-3dbd-4cb4-8494-0551dd701ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plot figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create first axis: plot close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(data1['Date'], data1['Close'], color='blue', label='Close Price')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price (S&P)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "ax2.plot(data1['Date'], data1['Volume'], color='darkgreen', label='Volume', alpha=0.7)\n",
    "ax2.set_ylabel('Volume', color='darkgreen')\n",
    "ax2.tick_params(axis='y', labelcolor='darkgreen')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of Close Price and Volume Over Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84fb19b-07b7-4d5f-a83d-3dd90c143233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming 'data' is your DataFrame and includes columns 'Date' and 'Volume'\n",
    "fig = px.line(data1, x='Date', y='Volume', title='S&P 500 Volume History')\n",
    "fig.update_traces(line=dict(color='green'))  # Change line color to green\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Volume',\n",
    "    xaxis_rangeslider_visible=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26c65-0904-4ee1-98bc-2344b6419e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming 'data' is your DataFrame and includes columns 'Date' and 'Close'\n",
    "fig = px.line(data1, x='Date', y='Close', title='S&P 500 Close Price History')\n",
    "fig.update_traces(line=dict(color='darkblue'))  # Change line color to dark blue\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Close Price (USD)',  # Correct y-axis title to match the plotted data\n",
    "    xaxis_rangeslider_visible=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd5eec-24ef-4111-abd9-af45d0fa70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate a 30-day moving average of the volume\n",
    "data1['Volume_MA30'] = data1['Volume'].rolling(window=30).mean()\n",
    "\n",
    "# Plot with both original volume and moving average\n",
    "fig = go.Figure()\n",
    "# Add the Volume line in green color\n",
    "fig.add_trace(go.Scatter(x=data1['Date'], y=data1['Volume'], mode='lines', name='Volume', line=dict(color='green')))\n",
    "# Add the 30-Day MA of Volume line in red color\n",
    "fig.add_trace(go.Scatter(x=data1['Date'], y=data1['Volume_MA30'], mode='lines', name='30-Day MA of Volume', line=dict(color='red')))\n",
    "fig.update_layout(title='S&P 500 Volume with 30-Day Moving Average', xaxis_title='Date', yaxis_title='Volume')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f18ab9-65e4-4253-9e20-f29c2b79321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Prepare year and month columns for the heatmap\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "\n",
    "# Pivot table for heatmap data\n",
    "heatmap_data = data.pivot_table(index='Year', columns='Month', values='Volume', aggfunc='mean')\n",
    "\n",
    "# Heatmap plot\n",
    "fig = px.imshow(heatmap_data, labels=dict(x=\"Month\", y=\"Year\", color=\"Average Volume\"),\n",
    "                title=\"Heatmap of Average Monthly Trading Volume\")\n",
    "fig.update_xaxes(side=\"bottom\")\n",
    "\n",
    "# Update layout to make the plot bigger\n",
    "fig.update_layout(\n",
    "    width=900,  # Width in pixels\n",
    "    height=600,  # Height in pixels\n",
    "    autosize=False  # Disable autosizing to use fixed dimensions\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e12e3-6b7e-4dcc-8a69-097aab244366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate a 30-day moving average of the close price\n",
    "data['Close_MA30'] = data['Close'].rolling(window=30).mean()\n",
    "\n",
    "# Create a plot with both original close price and moving average\n",
    "fig = go.Figure()\n",
    "# Add the Close Price line in dark blue color\n",
    "fig.add_trace(go.Scatter(x=data['Date'], y=data['Close'], mode='lines', name='Close Price', line=dict(color='darkblue')))\n",
    "# Add the 30-Day MA of Close line in red color\n",
    "fig.add_trace(go.Scatter(x=data['Date'], y=data['Close_MA30'], mode='lines', name='30-Day MA of Close', line=dict(color='red')))\n",
    "fig.update_layout(title='S&P 500 Close Price with 30-Day Moving Average', xaxis_title='Date', yaxis_title='Close Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b67fa-29c2-41c5-b0d2-af7da46f99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Prepare year and month columns for the heatmap\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "\n",
    "# Pivot table for heatmap data\n",
    "heatmap_data = data.pivot_table(index='Year', columns='Month', values='Close', aggfunc='mean')\n",
    "\n",
    "# Heatmap plot\n",
    "fig = px.imshow(heatmap_data, labels=dict(x=\"Month\", y=\"Year\", color=\"Close\"),\n",
    "                title=\"Heatmap of Average Monthly Close Price\")\n",
    "fig.update_xaxes(side=\"bottom\")\n",
    "\n",
    "# Update layout to make the plot bigger\n",
    "fig.update_layout(\n",
    "    width=900,  # Width in pixels\n",
    "    height=600,  # Height in pixels\n",
    "    autosize=False  # Disable autosizing to use fixed dimensions\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e37a79-edaa-4198-855f-2bc0d2151e6f",
   "metadata": {},
   "source": [
    "### Analyze Cboe volatility index VIX data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f99d2-732a-4725-be61-16ff7a2abb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59669c4d-b041-4565-9177-8bb7167b0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e3691-ce66-45dc-9360-2a20fbaa8683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead60a6c-e249-4044-a600-276e2f80e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "plt.plot(data2['date_VIX'], data2['close_VIX'], label='VIX Close Price',color='darkblue')\n",
    "plt.title('VIX Close Price History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price (USD)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b04916-2f97-4a0c-96ec-e713dfe0f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any conversion issues\n",
    "if data2['date_VIX'].isnull().any():\n",
    "    print(\"Warning: Some dates could not be converted and are NaT\")\n",
    "\n",
    "# Drop rows where the date could not be converted\n",
    "data2.dropna(subset=['date_VIX'], inplace=True)\n",
    "\n",
    "# Filter data from 2005 to 2010\n",
    "filtered_data2 = data2[(data2['date_VIX'] >= '2007-01-01') & (data['Date'] <= '2010-06-30')]\n",
    "\n",
    "# Select a subset of data for plotting (if needed, here we plot all filtered data)\n",
    "# subset_data = filtered_data.head(50)  # If you still want to limit to first 50 rows of the filtered data\n",
    "subset_data2 = filtered_data2.head(200)\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "subset_data2.set_index('date_VIX', inplace=True)\n",
    "\n",
    "# Plotting the prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(subset_data2.index, subset_data2['open_VIX'], label='Open Price', color='blue')\n",
    "plt.plot(subset_data2.index, subset_data2['high_VIX'], label='High Price', color='green')\n",
    "plt.plot(subset_data2.index, subset_data2['low_VIX'], label='Low Price', color='red')\n",
    "plt.plot(subset_data2.index, subset_data2['close_VIX'], label='Close Price', color='orange')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate by 45 degrees and align right\n",
    "plt.title('Price Comparison Over Time (2007-01-01, -2010-06-30)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb58a7-b363-4bab-84dd-f005d933864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame and includes columns 'Date' and 'Volume'\n",
    "fig = px.line(data2, x='date_VIX', y='close_VIX', title='VIX Close Price History')\n",
    "fig.update_layout(xaxis_title='date_VIX', yaxis_title='Volume_VIX', xaxis_rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21715c99-21a6-4d56-bfd3-33d48b309124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Extract month from Date for grouping in the plot\n",
    "data2['Month'] = data2['date_VIX'].dt.month\n",
    "\n",
    "# Box plot by month\n",
    "fig = px.box(data2, x='Month', y='close_VIX', title='Monthly Distribution of VIX Close Price History')\n",
    "fig.update_layout(xaxis_title='Month', yaxis_title='Close', xaxis_type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a01bae-22ad-4241-b433-4426d89e5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plot figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create first axis: plot close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(data2['date_VIX'], data2['close_VIX'], label='VIX Close Price',color='darkblue')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price (VIX)', color='darkblue')\n",
    "ax1.tick_params(axis='y', labelcolor='darkblue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "ax2.plot(data4['Date_USDX'],data4['close_USDX'], label='USDX Close Price',color='darkorange', alpha=0.7)\n",
    "ax2.set_ylabel('Volume', color='darkorange')\n",
    "ax2.tick_params(axis='y', labelcolor='darkorange')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of VIX and USDX Movement Close Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1192e-52c2-4b11-8f2d-af16aac49309",
   "metadata": {},
   "source": [
    "### Analyze Interest rate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef0a35-52ab-4e2b-a848-84f8c232a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8781b-585c-415b-b291-d52ba3b61264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Extract month from Date for grouping in the plot\n",
    "data3['Month'] = data3['DATE'].dt.month\n",
    "\n",
    "# Box plot by month\n",
    "fig = px.box(data3, x='Month', y='EFFR', title='Monthly Distribution of EFFR History')\n",
    "fig.update_layout(xaxis_title='Month', yaxis_title='EFFR', xaxis_type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cf589-74a7-42cf-aa0e-7fc60d7f2ad6",
   "metadata": {},
   "source": [
    "### Analyze US dollar index Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89f8b0-7d3c-49d3-bd2a-0ffe9e869852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c426689-88b2-46ce-8284-abe20dc738ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed515eb1-e4a8-442b-97da-72537209ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closing price history\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data4['Date_USDX'],data4['close_USDX'], label='US Dollar index Close Price')\n",
    "plt.title('USDX_ Close Price History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('US Dollar index')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ace8a9-19e8-47d7-a2ba-1914b255d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a figure with a secondary y-axis\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces, one for the closing prices and another for the volume\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data4['Date_USDX'], y=data4['close_USDX'], name='Close USDX',\n",
    "               mode='lines', line=dict(color='blue'), yaxis='y1')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data4['Date_USDX'], y=data4['Volume_USDX'], name='Volume USDX',\n",
    "               mode='lines', line=dict(color='red'), yaxis='y2')\n",
    ")\n",
    "\n",
    "# Create axis objects\n",
    "fig.update_layout(\n",
    "    title='Comparison of USDX Close Price and Volume',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Close Price USDX', color='blue', side='left'),\n",
    "    yaxis2=dict(title='Volume USDX', color='red', side='right', overlaying='y', anchor='x')\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638c5ca-1ecc-44d8-9401-0c66bfe82bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a candlestick chart\n",
    "fig = go.Figure(data=[go.Candlestick(\n",
    "    x=data4['Date_USDX'],\n",
    "    open=data4['open_USDX'],\n",
    "    high=data4['high_USDX'],\n",
    "    low=data4['low_USDX'],\n",
    "    close=data4['close_USDX'],\n",
    "    name=\"USDX Prices\"\n",
    ")])\n",
    "\n",
    "# Overlay Volume on the secondary y-axis\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=data4['Date_USDX'], \n",
    "        y=data4['Volume_USDX'], \n",
    "        name='Volume USDX', \n",
    "        yaxis=\"y2\",\n",
    "        line=dict(width=2, color='blue')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title=\"US Dollar Index (USDX) Prices and Volume\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Price\",\n",
    "    legend_title=\"Legend\",\n",
    "    xaxis_rangeslider_visible=False,  # Disable range slider for better clarity\n",
    "    yaxis=dict(\n",
    "        side='left'  # Price on the left y-axis\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Volume\",\n",
    "        overlaying='y',  # Overlay on the same x-axis but different scale\n",
    "        side='right',   # Volume on the right y-axis\n",
    "        showgrid=False  # Disable gridlines for secondary y-axis to reduce clutter\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c913e0a-6b94-45dc-908b-016c05c35838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c286cda-9692-4c38-9aaa-a3b85caa3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create the first axis: plot close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get the current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(data4['Date_USDX'], data4['close_USDX'], color='darkblue', label='Close USDX')\n",
    "ax1.set_xlabel('Date_USDX')\n",
    "ax1.set_ylabel('Close Price (USD)', color='darkblue')\n",
    "ax1.tick_params(axis='y', labelcolor='darkblue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "ax2.plot(data4['Date_USDX'], data4['Volume_USDX'], color='green', label='Volume USDX', alpha=0.7)\n",
    "ax2.set_ylabel('Volume USDX', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of USDX Close Price and Volume Over Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d704f05-b400-4a73-aa57-9a5e3fb652b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pandas as pd\n",
    "\n",
    "# Set the plotting backend to Matplotlib\n",
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "\n",
    "# Ensure 'Date_USDX' is the index, if not, set it\n",
    "if not data4.index.name == 'Date_USDX':\n",
    "    data4.set_index('Date_USDX', inplace=True)\n",
    "\n",
    "# Perform time series decomposition on 'Volume_USDX'\n",
    "# Assume an annual cycle; adjust the 'period' if your data frequency is different\n",
    "decomposition = seasonal_decompose(data4['Volume_USDX'], model='additive', period=12)\n",
    "\n",
    "# Plotting the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 8))\n",
    "decomposition.observed.plot(ax=ax1)\n",
    "ax1.set_title('Observed')\n",
    "decomposition.trend.plot(ax=ax2)\n",
    "ax2.set_title('Trend')\n",
    "decomposition.seasonal.plot(ax=ax3)\n",
    "ax3.set_title('Seasonal')\n",
    "decomposition.resid.plot(ax=ax4)\n",
    "ax4.set_title('Residual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cab54-7a36-4626-83d2-be5088e257be",
   "metadata": {},
   "source": [
    "### Analyze Civilian unemployment rate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331f439-79dc-4eb7-b83f-8e3a949083d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Bar chart for UNRATE\n",
    "fig = px.bar(data5, x='DATE', y='UNRATE', title='Unemployment Rate Over Time')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Unemployment Rate (%)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37190b-d7b2-4d4f-919a-cf345630469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closing price history\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data5['DATE'],data5['UNRATE'], label='Unemployment Rate Over Time')\n",
    "plt.title('Consumer sentiment History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Consumer sentiment')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming 'umsent_data' is your DataFrame with the index set to date and it includes a 'UMCSENT' column\n",
    "fig = px.line(data5, y='UNRATE', title='U.S. Consumer Sentiment Index Over Time')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Index Value', xaxis_rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088b3d4-e601-4564-ac97-76056937b4ff",
   "metadata": {},
   "source": [
    "### Analyze Consumer sentiment index Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77d17e-d5ad-4448-b1d8-5928e703b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closing price history\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data6['DATE'],data6['UMCSENT'], label='Consumer sentiment')\n",
    "plt.title('Consumer sentiment History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Consumer sentiment')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming 'umsent_data' is your DataFrame with the index set to date and it includes a 'UMCSENT' column\n",
    "fig = px.line(data6, y='UMCSENT', title='U.S. Consumer Sentiment Index Over Time')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Index Value', xaxis_rangeslider_visible=True)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cf667-59c0-4860-ac07-691470de06d1",
   "metadata": {},
   "source": [
    "### Analyze Apple data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418baa3-95da-4b93-a24e-fcff9da1d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf1d79-3a27-48f3-b809-ebca6f23b221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50e516-1ae5-4b00-8d46-67bd8e5906fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d76500-fd63-437f-8bde-0146398eedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closing price history\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data7['Date'],data7['close_AAPL'], label='Apple Close Price')\n",
    "plt.title('Apple Close Price History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price (Apple)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Optionally, plot the MACD and Signal Line\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data7['Date'], data7['MACD'], label='MACD', color='blue')\n",
    "plt.plot(data7['Date'], data7['Signal_Line'], label='Signal Line', color='orange')\n",
    "plt.title('MACD and Signal Line for AAPL')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b2561-d901-46b5-b359-640c9372baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Line plot for Trading Volume\n",
    "fig = px.line(data7, x='Date', y='Volume_AAPL', title='AAPL Trading Volume History')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Volume')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81e72d-c3dd-40e3-a164-977f7d7b7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# Line plot for ATR\n",
    "fig = px.line(data7, x='Date', y='close_AAPL', title='Close price')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='close_AAPL')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49f6bb-732e-48be-910c-e563b0a5cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the plot figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create the first axis: plot close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get the current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Check if 'Date' is the index and adjust plotting accordingly\n",
    "x_values = data7.index if 'Date' not in data7.columns else data7['Date']\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(x_values, data7['close_AAPL'], color='blue', label='Close Price AAPL')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price (USD)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "ax2.plot(x_values, data7['Volume_AAPL'], color='green', label='Volume AAPL', alpha=0.7)\n",
    "ax2.set_ylabel('Volume', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of AAPL Close Price and Volume Over Time')\n",
    "plt.show()\n",
    "# Set up the plot figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create the first axis: plot close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get the current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Check if 'Date' is the index and adjust plotting accordingly\n",
    "x_values = data7.index if 'Date' not in data7.columns else data7['Date']\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(x_values, data7['close_AAPL'], color='blue', label='Close Price AAPL')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price (USD)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "ax2.plot(x_values, data7['Volume_AAPL'], color='green', label='Volume AAPL', alpha=0.7)\n",
    "ax2.set_ylabel('Volume', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of AAPL Close Price and Volume Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d36ee0-9110-4525-aa2c-4cff903261fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plot figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create the first axis: plot close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get the current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "#ax3 = ax1.twinx() \n",
    "\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(data1['Date'], data1['Close'], color='red', label='Close USDX')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price s&p 500', color='red')\n",
    "ax1.tick_params(axis='y', labelcolor='red')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "ax2.plot(data7['Date'], data7['close_AAPL'], color='blue', label='close_AAPL', alpha=0.7)\n",
    "ax2.set_ylabel('Close Price (USD)', color='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Plot the Volume data on the second y-axis\n",
    "#ax3.plot(data2['date_VIX'], data2['close_VIX'], color='green', label='Volume USDX', alpha=0.7)\n",
    "#ax3.set_ylabel('Close Price (USD)', color='blue')\n",
    "#ax3.tick_params(axis='y', labelcolor='green')\n",
    "#ax3.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of USDX Close Price and Volume Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6fa82-8f7d-463d-8cda-6bef1a27aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot figure\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Create additional y-axes\n",
    "ax2 = ax1.twinx()  # Second y-axis sharing the same x-axis\n",
    "ax3 = ax1.twinx()  # Third y-axis sharing the same x-axis\n",
    "\n",
    "# Offset the third axis to avoid overlap\n",
    "ax3.spines[\"right\"].set_position((\"outward\", 60))\n",
    "\n",
    "# Plot the Close price data for S&P 500 on the first y-axis\n",
    "ax1.plot(data1['Date'], data1['Close'], color='purple', label='Close S&P 500')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price S&P 500', color='purple')\n",
    "ax1.tick_params(axis='y', labelcolor='purple')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the Close price data for AAPL on the second y-axis\n",
    "ax2.plot(data7['Date'], data7['close_AAPL'], color='green', label='Close AAPL', alpha=0.7)\n",
    "ax2.set_ylabel('Close Price AAPL (USD)', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Plot the Close price data for VIX on the third y-axis\n",
    "ax3.plot(data2['date_VIX'], data2['close_VIX'], color='orange', label='Close VIX', alpha=0.7)\n",
    "ax3.set_ylabel('Close Price VIX (USD)', color='orange')\n",
    "ax3.tick_params(axis='y', labelcolor='orange')\n",
    "ax3.legend(loc='lower right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of Close Prices Over Time')\n",
    "ax1.grid(True)  # Adding grid for better readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b44ee-8461-4672-900f-6b2408c4bbfa",
   "metadata": {},
   "source": [
    "# PreProcces Data sets For start Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67eed3-aa23-44cf-9aef-cc3d532a0a9d",
   "metadata": {},
   "source": [
    "## Fetching Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2297f-704c-4b39-b714-ab16ab0ca817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical data for the S&P US\n",
    "DATA1 = yf.download(\"^GSPC\" , start=\"2001-01-01\", end=\"2024-05-01\")\n",
    "# Fetch historical data for the VIX\n",
    "DATA2 = yf.download(\"^VIX\", start=\"2001-01-01\", end=\"2024-05-01\")\n",
    "# Fetch historical data for the U.S. Dollar Index\n",
    "DATA3 = yf.download(\"DX=F\", start=\"2001-01-01\", end=\"2024-05-01\")\n",
    "# Download historical data for Apple Inc.\n",
    "DATA4 = yf.download('AAPL', start='2001-01-01', end='2024-05-01')\n",
    "\n",
    "\n",
    "start_date = datetime.datetime(2001, 1, 2)\n",
    "end_date = datetime.datetime(2024, 5, 30)\n",
    "\n",
    "# Fetch the data from FRED\n",
    "DATA5 = pdr.get_data_fred('EFFR', start_date, end_date)\n",
    "# Fetch the data UNRATE\n",
    "DATA6 = pdr.get_data_fred('UNRATE', start_date, end_date)\n",
    "# Fetch the data UMCSENT\n",
    "DATA7 = pdr.get_data_fred('UMCSENT', start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c559be-cf62-4099-9d59-843c8c6bd4e3",
   "metadata": {},
   "source": [
    "## Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b5dfe-5940-4b20-bb85-f3a9082f212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns DATA2\n",
    "DATA2.rename(columns={\n",
    "   'Date': 'date_VIX',\n",
    "   'Open': 'open_VIX',\n",
    "   'High': 'high_VIX',\n",
    "   'Low': 'low_VIX',\n",
    "   'Close': 'close_VIX',\n",
    "   'Adj Close': 'adj_close_VIX',\n",
    "   'Volume' : 'Volume_VIX'\n",
    "}, inplace=True)\n",
    "# Rename the columns DATA3\n",
    "DATA3.rename(columns={\n",
    "    'Date' :'Date_USDX',\n",
    "    'Open': 'open_USDX',\n",
    "    'High': 'high_USDX',\n",
    "    'Low': 'low_USDX',\n",
    "    'Close': 'close_USDX',\n",
    "    'Adj Close': 'Adj_USDX',\n",
    "    'Volume': 'Volume_USDX'\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename the columns DATA4\n",
    "DATA4.rename(columns={\n",
    "    'Open': 'open_AAPL',\n",
    "    'High': 'high_AAPL',\n",
    "    'Low': 'low_AAPL',\n",
    "    'Close': 'close_AAPL',\n",
    "    'Adj Close': 'Adj_Close_APPL',\n",
    "    'Volume': 'Volume_AAPL'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000bb5a7-daa3-4c8b-8e7e-54da4f0887a3",
   "metadata": {},
   "source": [
    "## Adding Average price column and Percentage Markup to Prices columns DATA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b436c05-adf9-4805-9654-d06db50e760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA1['AVG_price'] = DATA1[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
    "\n",
    "DATA1['Price_Change'] = DATA1['Close'].pct_change()\n",
    "DATA1['Volume_Change'] = DATA1['Volume'].pct_change()\n",
    "DATA1['Price_AVG_Change'] = DATA1['AVG_price'].pct_change()\n",
    "\n",
    "\n",
    "# Calculate True Range (TR)\n",
    "DATA1['H-L'] = DATA1['High'] - DATA1['Low']\n",
    "DATA1['H-PC'] = abs(DATA1['High'] - DATA1['Close'].shift(1))\n",
    "DATA1['L-PC'] = abs(DATA1['Low'] - DATA1['Close'].shift(1))\n",
    "DATA1['TR'] = DATA1[['H-L', 'H-PC', 'L-PC']].max(axis=1)\n",
    "\n",
    "# Calculate ATR using the correct DataFrame\n",
    "atr_period = 14  # Typical period used for ATR\n",
    "DATA1['ATR'] = DATA1['TR'].rolling(atr_period).mean()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the MACD and Signal Line indicators\n",
    "# Short term exponential moving average (EMA)\n",
    "Exp1 = DATA1['Close'].ewm(span=12, adjust=False).mean()\n",
    "# Long term exponential moving average (EMA)\n",
    "Exp2 = DATA1['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "\n",
    "# MACD line\n",
    "DATA1['MACD'] = Exp1 - Exp2\n",
    "# Signal line\n",
    "DATA1['Signal_Line'] = DATA1['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate price changes\n",
    "DATA1['Delta'] = DATA1['Close'].diff()\n",
    "\n",
    "# Calculate gains and losses\n",
    "DATA1['Gain'] = DATA1['Delta'].where(DATA1['Delta'] > 0, 0)\n",
    "DATA1['Loss'] = DATA1['Delta'].where(DATA1['Delta'] < 0, 0)\n",
    "\n",
    "# Calculate average gain and loss\n",
    "window = 14  # Typical period for RSI\n",
    "DATA1['Avg Gain'] = DATA1['Gain'].rolling(window=window, min_periods=1).mean()\n",
    "DATA1['Avg Loss'] = DATA1['Loss'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "# Calculate RS and RSI\n",
    "DATA1['RS'] = DATA1['Avg Gain'] / (DATA1['Avg Loss'] + 1e-10) \n",
    "DATA1['RSI'] = 100 - (100 / (1 + DATA1['RS']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac4fa7-e22d-4c6a-8983-f14a1c7a02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d789c4-f45d-45ac-b4dc-2b0efe08cfa8",
   "metadata": {},
   "source": [
    "## Adding MACD AND Signal Line indicators DATA4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14daca3c-7cb9-47e1-be5f-d0aca72e95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MACD and Signal Line indicators\n",
    "# Short term exponential moving average (EMA)\n",
    "Exp1_A = DATA4['close_AAPL'].ewm(span=12, adjust=False).mean()\n",
    "# Long term exponential moving average (EMA)\n",
    "Exp2_A = DATA4['close_AAPL'].ewm(span=26, adjust=False).mean()\n",
    "# MACD line\n",
    "DATA4['MACD_A'] = Exp1_A - Exp2_A\n",
    "# Signal line\n",
    "DATA4['Signal_Line_A'] = DATA4['MACD_A'].ewm(span=9, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b48e7-824c-4028-9d74-0adcf6c02ab3",
   "metadata": {},
   "source": [
    "## Adding True Range (TR) and (ATR) DATA4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad416bd-c72b-4a57-ac49-03975704281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate True Range (TR)\n",
    "DATA4['H-L_A'] = DATA4['high_AAPL'] - DATA4['low_AAPL']\n",
    "DATA4['H-PC_A'] = abs(DATA4['high_AAPL'] - DATA4['close_AAPL'].shift(1))\n",
    "DATA4['L-PC_A'] = abs(DATA4['low_AAPL'] - DATA4['close_AAPL'].shift(1))\n",
    "DATA4['TR_A'] = DATA4[['H-L_A', 'H-PC_A', 'L-PC_A']].max(axis=1)\n",
    "\n",
    "# Calculate ATR using the correct DataFrame\n",
    "atr_period = 14  # Typical period used for ATR\n",
    "DATA4['ATR_A'] = DATA4['TR_A'].rolling(atr_period).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e93d3-c440-409b-b33f-a16fefc2eb20",
   "metadata": {},
   "source": [
    "## Adding Delta, gains, losses, RS and RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6dc9a-3f4b-45e4-acf5-7a30c48e0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price changes\n",
    "DATA4['Delta_A'] = DATA4['close_AAPL'].diff()\n",
    "\n",
    "# Calculate gains and losses\n",
    "DATA4['Gain_A'] = DATA4['Delta_A'].where(DATA4['Delta_A'] > 0, 0)\n",
    "DATA4['Loss_A'] = DATA4['Delta_A'].where(DATA4['Delta_A'] < 0, 0)\n",
    "\n",
    "# Calculate average gain and loss\n",
    "window = 14  # Typical period for RSI\n",
    "DATA4['Avg Gain_A'] = DATA4['Gain_A'].rolling(window=window, min_periods=1).mean()\n",
    "DATA4['Avg Loss_A'] = DATA4['Loss_A'].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "# Calculate RS and RSI\n",
    "DATA4['RS_A'] = DATA4['Avg Gain_A'] / (DATA4['Avg Loss_A'] + 1e-10) \n",
    "DATA4['RSI_A'] = 100 - (100 / (1 + DATA4['RS_A']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7fe05-3aef-41c4-9274-8ffe5c9e1b3c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f377b3a-eaad-41ae-8a07-e3ac0c65d1fc",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1008a-bd32-4d2c-9f50-2677b454f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4583f2-3f2a-4f85-9f06-61e82ea5b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the median of each column\n",
    "for col in DATA1.columns:\n",
    "    if pd.api.types.is_numeric_dtype(DATA1[col]):  # Check if the column is numeric\n",
    "        DATA1[col].fillna(DATA1[col].median(), inplace=True)\n",
    "\n",
    "# Check for missing values after filling\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(DATA1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf3a99-1dce-44bf-b569-7a7c81371283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c3e65-04d8-4ce6-a4b5-31517968b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA3.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc02c0-82b8-45d0-9484-a5026272afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA4.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ea848-7e60-4ab3-9c12-cbb030e00e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of each column\n",
    "DATA4_filled_mean = DATA4.fillna(DATA4.mean())\n",
    "print(\"Missing values after filling with mean:\")\n",
    "print(DATA4_filled_mean.isnull().sum())\n",
    "\n",
    "# Alternatively, fill with median or mode\n",
    "DATA4_filled_median = DATA4.fillna(DATA4.median())\n",
    "DATA4_filled_mode = DATA4.fillna(DATA4.mode().iloc[0])\n",
    "DATA4 = DATA4_filled_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994242f-535b-4927-a650-8590e9f9d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA5.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a956cb7-faa3-4b41-9d17-0cdda260b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with any missing values\n",
    "DATA5 = DATA5.dropna()\n",
    "print(DATA5.isnull().sum())\n",
    "# Print the number of rows after removal to understand the impact\n",
    "print(\"Number of rows after removing missing values:\", len(DATA5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a64c9-afe3-4f39-9cde-f934a7e68edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA6.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f77fb6-32cf-453c-813e-c901482e4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA7.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ead0ba-e451-4c2c-9f7d-d049277df724",
   "metadata": {},
   "source": [
    "## Correlation Matrix Heatmap DATA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703ae6d-c458-42d0-9726-4e52f559423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns for correlation matrix\n",
    "numeric_df = DATA1.select_dtypes(include=['float64', 'int64', 'int32'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedce104-550e-4d21-8728-05145eddaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding specific columns\n",
    "excluded_columns = ['Date', 'year_month']  # Adjust if these columns are present or named differently\n",
    "features = [col for col in DATA1.columns if col not in excluded_columns]\n",
    "\n",
    "# Determining the number of rows/columns for the subplot grid\n",
    "n = len(features)\n",
    "cols = 3  # Set the number of columns to 3 as mentioned\n",
    "rows = n // cols + (n % cols > 0)  # Calculate rows needed based on the number of features\n",
    "\n",
    "plt.figure(figsize=(5 * cols, 5 * rows))  # Adjust overall figure size if necessary\n",
    "\n",
    "# Plotting each feature against 'Close' price with a linear regression fit\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    sns.regplot(x=DATA1[feature], y=DATA1['Close'], order=1, line_kws={\"color\": \"red\"}, scatter_kws={\"s\": 15, \"color\": \"blue\"})\n",
    "    plt.title(f'{feature} vs Close')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Close')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d83ad-f74b-4cf0-bfc8-a98640449bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_df = DATA1.select_dtypes(include=['float64', 'int64', 'int8'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05deb013-8447-4f19-afce-7af0042f0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correlations with 'close' column\n",
    "correlations_close = correlation_matrix['Close'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations with 'close'\n",
    "print(\"Correlations with close:\")\n",
    "print(correlations_close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5fa00-e2af-4ae0-8d4a-b6631d1b023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a bar plot with bars colored in dark blue\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=correlations_close.values, y=correlations_close.index, color='darkblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Correlation with Close')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Correlations with Close Column')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632c37d-a295-49b9-8191-98bb274c46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_close.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704abd00-2bef-40a5-a282-9f2437f8d93e",
   "metadata": {},
   "source": [
    "## Correlation Matrix Heatmap DATA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd39095-e3d9-43b6-a9b2-c822187b71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Volume_VIX' column from DATA2\n",
    "DATA2 = DATA2.drop(columns=['Volume_VIX'])\n",
    "# Verify that the column has been removed\n",
    "print(DATA2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775da352-d010-4a61-8634-ec239afc3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming DATA2 is your DataFrame and it contains the specified columns\n",
    "# Select only the numeric columns for correlation matrix\n",
    "numeric_df2 = DATA2.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix for the numeric columns\n",
    "corr_matrix2 = numeric_df2.corr()\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(6, 6))  # Adjusted the figure size for better visibility\n",
    "sns.heatmap(corr_matrix2, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap for DATA2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1c52b-4a2e-4a1f-8762-7198e09df73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8ce32-5765-4087-8540-3f644ea54d37",
   "metadata": {},
   "source": [
    "## Correlation Matrix Heatmap DATA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3203f99-cc43-44d0-ba2a-386b110f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numeric_df3 = DATA3.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix3 = numeric_df3.corr()\n",
    "\n",
    "# Plot the heatmap for the correlation matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(corr_matrix3, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap for DATA3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5834e-7a53-49ee-b473-893d1b4b7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989bd69f-2904-4e8c-b85f-a0e3124d817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correlations with 'close' column\n",
    "correlations_close4 = corr_matrix3['close_USDX'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations with 'close'\n",
    "print(\"Correlations with close:\")\n",
    "print(correlations_close4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62ce0a-de7a-465c-8ea3-b68cc3ef67a4",
   "metadata": {},
   "source": [
    "## Correlation Matrix Heatmap DATA4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cb400-0084-471b-96fc-b11cea7766b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming DATA4 is your DataFrame and contains the specified columns\n",
    "# Select only the numeric columns for the correlation matrix\n",
    "numeric_df4 = DATA4.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix4 = numeric_df4.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 15))  # Adjusting the figure size for better readability\n",
    "sns.heatmap(corr_matrix4, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap for DATA4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a45817-e662-4870-8fcc-1422f0cae519",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea001d0-c0d6-4ddc-b46d-f80a4b4acdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7483b-31a5-49cb-bfe1-64becb2ceda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correlations with 'close' column\n",
    "correlations_close4 = corr_matrix4['close_AAPL'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations with 'close'\n",
    "print(\"Correlations with close:\")\n",
    "print(correlations_close4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbb9fb-6def-45df-96ce-ca94a19a4cc5",
   "metadata": {},
   "source": [
    "# Concat Final data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566b275-6624-4e65-9565-ecaa314aaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all DataFrames have the same index or are aligned by some key\n",
    "DATA12 = pd.concat([DATA1, DATA2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a9cf4-f14a-4b2a-8097-274883d5dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all DataFrames have the same index or are aligned by some key\n",
    "DATA123 = pd.concat([DATA12, DATA3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcae0e7-9a0d-4f98-abb1-03769687052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA123.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4c250-e0f8-47cc-a50f-36932265c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where 'Open' column has missing values\n",
    "missing_open_rows = DATA123[DATA123['open_USDX'].isnull()]\n",
    "\n",
    "print(missing_open_rows.index.tolist())\n",
    "\n",
    "# Optionally, display the rows with missing 'Open' values\n",
    "print(\"\\nRows with missing 'Open' values:\")\n",
    "print(missing_open_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff938ef0-b202-4212-8d03-d053571f9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows where 'Open' column has missing values\n",
    "missing_open_rows2 = DATA123[DATA123['Open'].isnull()]\n",
    "\n",
    "print(missing_open_rows2.index.tolist())\n",
    "\n",
    "# Optionally, display the rows with missing 'Open' values\n",
    "print(\"\\nRows with missing 'Open' values:\")\n",
    "print(missing_open_rows2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab56eb7-e682-45ec-8c9d-73e386a7e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows with any missing values\n",
    "cleaned_data123 = DATA123.dropna()\n",
    "print(cleaned_data123.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0eef4-93c9-409d-8800-fef530ab5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA123=cleaned_data123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74bf8b-f3fb-4656-b645-2673b916cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming all DataFrames have the same index or are aligned by some key\n",
    "DATA1234 = pd.concat([DATA123, DATA4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220219a-3332-4ce9-9e97-2df8f9b7587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(DATA1234.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badf1b9-c9b3-4489-b495-544946119d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data1234 = DATA1234.dropna()\n",
    "print(cleaned_data1234.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763542a6-0620-4b2f-9a46-e2aebda44f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA1234=cleaned_data1234\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa598c29-312e-4eaf-988e-ef8e7a32a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all DataFrames have the same index or are aligned by some key\n",
    "DATA12345 = pd.concat([DATA1234, DATA5], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6cd67-591e-40f1-945c-442808ba7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA12345.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33aa11-0959-491f-bddf-815908ea8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data12345 = DATA12345.dropna()\n",
    "print(cleaned_data12345.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f3308-4b18-4aee-bcfb-0819dcd87e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA12345= cleaned_data12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220949fb-5245-4d55-94bb-526fb8319779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'DATE' is a datetime index\n",
    "DATA6['DATE'] = pd.to_datetime(DATA6.index)\n",
    "\n",
    "# Set 'DATE' as the DataFrame index if it's not already\n",
    "DATA6.set_index('DATE', inplace=True)\n",
    "\n",
    "# Resample to daily frequency, using forward fill to propagate the last valid observation\n",
    "daily_DATA6 = DATA6.resample('D').ffill()\n",
    "\n",
    "# Display the first few rows to confirm daily data\n",
    "DATA6 = daily_DATA6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9327e65-3741-4612-a390-2bb6b14e5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'DATE' is a datetime index\n",
    "DATA7['DATE'] = pd.to_datetime(DATA7.index)\n",
    "\n",
    "# Set 'DATE' as the DataFrame index if it's not already\n",
    "DATA7.set_index('DATE', inplace=True)\n",
    "\n",
    "# Resample to daily frequency, using forward fill to propagate the last valid observation\n",
    "daily_DATA7 = DATA7.resample('D').ffill()\n",
    "\n",
    "# Display the first few rows to confirm daily data\n",
    "DATA7 = daily_DATA7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9471164-bc0d-45ff-81c2-d9a8abfaf319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all DataFrames have the same index or are aligned by some key\n",
    "DATA67 = pd.concat([DATA6, DATA7], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875a8a7-ee7c-4d57-8e17-4c74d536acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA67.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940057c-18a2-44ec-a872-e8b06ec041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming DATA12345 and DATA67 are your DataFrames and they have the same index or are aligned by some key\n",
    "pd = pd.concat([DATA12345, DATA67], axis=1)\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7cbe9-91a1-4115-9688-951267253ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000da1a6-1a5b-4f1b-bd2f-d99e55a82a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dp = pd.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cecfbf-12d5-4267-b562-04b2ce81b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_dp.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339a465-e357-4af8-8057-232531b2aa0f",
   "metadata": {},
   "source": [
    "# Final Data = DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba80595-a99c-45d8-830e-562007200814",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = clean_dp\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e343eb0-df1a-412a-9f8d-cd4ecb279dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fbf9f-17a2-4e20-bdf1-737bf1cdea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa64c00-32ad-41f0-be0e-e85d46d8fa65",
   "metadata": {},
   "source": [
    "## Comparison of Close Price and UNRATE Visualation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab67927-f51e-4be3-90ba-90808c4f693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure and axis\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create first axis: plot Close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(DF.index, DF['Close'], color='blue', label='Close Price')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the EFFR data on the second y-axis\n",
    "ax2.plot(DF.index, DF['UNRATE'], color='red', label='UNRATE', alpha=0.7)\n",
    "ax2.set_ylabel('UNRATE', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of Close Price and UNRATE Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e064dc-0e49-4ed5-b9e3-685463576e2b",
   "metadata": {},
   "source": [
    "## Comparison of Close Price and UMCSENT Visualation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb2257-50f2-44a3-bbbf-7582085fc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure and axis\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create first axis: plot Close prices on the left y-axis\n",
    "ax1 = plt.gca()  # Get current axis\n",
    "ax2 = ax1.twinx()  # Create another axis that shares the same x-axis\n",
    "\n",
    "# Plot the Close price data\n",
    "ax1.plot(DF.index, DF['Close'], color='blue', label='Close Price')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Close Price', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot the EFFR data on the second y-axis\n",
    "ax2.plot(DF.index, DF['UMCSENT'], color='red', label='UMCSENT', alpha=0.7)\n",
    "ax2.set_ylabel('UMCSENT', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Comparison of Close Price and UMCSENT Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a695ecd-e780-4ba3-a37c-693adde51e0f",
   "metadata": {},
   "source": [
    "## Compute the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963f90e-7d9b-4de6-a8cf-5e3386808139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only the numeric columns for the correlation matrix\n",
    "numeric_DF = DF.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix_DF = numeric_DF.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(30, 30))  # Adjusting the figure size for better readability\n",
    "sns.heatmap(corr_matrix_DF, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap for DATA4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c0242-3169-46d5-9516-2ee12163176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fa18d-238f-4b14-a378-a0d4d8b2643d",
   "metadata": {},
   "source": [
    "## Plotting each feature against Close price with a linear regression fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23501d-07e3-418f-8030-775cdb46e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exclude non-numeric columns that are not relevant for regression plots\n",
    "excluded_columns_DF = ['Date', 'year_month']  # Adjust this if you have different non-numeric columns\n",
    "\n",
    "# Using all numeric columns except for excluded ones\n",
    "features_DF = [col for col in DF.columns if col not in excluded_columns_DF]\n",
    "\n",
    "# Determining the number of rows/columns for the subplot grid\n",
    "n = len(features_DF)\n",
    "cols = 6  # Set the number of columns\n",
    "rows = n // cols + (n % cols > 0)  # Calculate rows needed based on number of features\n",
    "\n",
    "plt.figure(figsize=(5 * cols, 5 * rows))  # Adjust overall figure size if necessary\n",
    "\n",
    "# Plotting each feature against 'close_AAPL' price with a linear regression fit\n",
    "for i, feature in enumerate(features_DF):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    sns.regplot(x=DF[feature], y=DF['Close'], order=1, line_kws={\"color\": \"red\"}, scatter_kws={\"s\": 15, \"color\": \"blue\"})\n",
    "    plt.title(f'{feature} vs Close Price')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Close')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd117ad-643f-4946-88c4-47d5e04a7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_DF = DF.corr()\n",
    "corr_matrix_DF[\"Close\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19412f87-b5ef-43bc-bb1d-8f8bdf9bbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = DF.corr()\n",
    "\n",
    "# Get the correlation of all features with 'Close', sort them, and drop 'Close' itself\n",
    "corr_with_Close = corr_matrix_DF[\"Close\"].sort_values(ascending=False).drop('Close')\n",
    "\n",
    "# Normalize the correlation coefficients to use with the colormap\n",
    "norm = plt.Normalize(corr_with_Close.min(), corr_with_Close.max())\n",
    "colors = plt.cm.coolwarm(norm(corr_with_Close.values))  # coolwarm colormap for diverging values\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x=corr_with_Close.values, y=corr_with_Close.index, palette=colors)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Correlation Coefficients with Close')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33ed8e-5bbb-4823-9695-09b5f21088a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = DF.corr()\n",
    "\n",
    "# Get the correlation of all features with 'Close', sort them in descending order\n",
    "positive_correlations = corr_matrix['Close'].sort_values(ascending=False)\n",
    "\n",
    "# Filter to show only positive correlations\n",
    "positive_correlations = positive_correlations[positive_correlations > 0]\n",
    "\n",
    "# Print the positive correlations with 'Close'\n",
    "print(\"Positive correlations with 'Close':\")\n",
    "print(positive_correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366ae23-ad5f-4f3a-a938-5b824c060c82",
   "metadata": {},
   "source": [
    "# Feature Importances from Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031dddad-37a7-45f8-825c-119f51a2d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the features based on your dataset structure\n",
    "features = [\n",
    "    'Open', 'High', 'Low', 'Adj Close', 'Volume',\n",
    "    'AVG_price', 'Price_Change', 'Volume_Change', 'Price_AVG_Change',\n",
    "    'H-L', 'H-PC', 'L-PC', 'TR', 'ATR', 'MACD', 'Signal_Line',\n",
    "    'Delta', 'Gain', 'Loss', 'Avg Gain', 'Avg Loss', 'RS', 'RSI',\n",
    "    'open_VIX', 'high_VIX', 'low_VIX', 'close_VIX', 'adj_close_VIX',\n",
    "    'open_USDX', 'high_USDX', 'low_USDX', 'close_USDX', 'Adj_USDX', 'Volume_USDX',\n",
    "    'open_AAPL', 'high_AAPL', 'low_AAPL', 'Adj_Close_APPL', 'Volume_AAPL',\n",
    "    'MACD_A', 'Signal_Line_A', 'H-L_A', 'ATR_A', 'TR_A',\n",
    "    'H-PC_A', 'L-PC_A', 'Delta_A', 'Gain_A', 'Loss_A',\n",
    "    'Avg Gain_A', 'Avg Loss_A', 'RS_A', 'RSI_A',\n",
    "    'EFFR', 'UNRATE', 'UMCSENT'\n",
    "]\n",
    "\n",
    "target = 'Close'  # Assuming 'Close' is the target variable you want to predict\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = DF[features]\n",
    "y = DF[target]\n",
    "\n",
    "# Handle NaN and Inf values\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infinities with NaN\n",
    "X.fillna(X.mean(), inplace=True)  # Fill NaNs with the mean of each column\n",
    "y.fillna(y.mean(), inplace=True)  # Assuming 'y' could also have missing values\n",
    "\n",
    "# Train the Random Forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to show feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042db30-bd1d-4338-9d67-8e861b1047f1",
   "metadata": {},
   "source": [
    "# Shape Value for features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07c9bb-bc18-4eb5-9666-67917427602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5750cf-87a7-4c7b-ae7c-0d92438b99c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0276668e-ea6b-4837-9089-a3fc69129a20",
   "metadata": {},
   "source": [
    "### RandomForestRegressor Shape Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580f1db-3463-4ed4-bb07-0e583dedaaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize JavaScript visualization in the notebook\n",
    "shap.initjs()\n",
    "\n",
    "# Your dataset 'DF' head\n",
    "DF.head()\n",
    "\n",
    "# Features and target variable setup\n",
    "features = [\n",
    "    'Open', 'High', 'Low', 'Adj Close', 'Volume',\n",
    "    'AVG_price', 'Price_Change', 'Volume_Change', 'Price_AVG_Change',\n",
    "    'H-L', 'H-PC', 'L-PC', 'TR', 'ATR', 'MACD', 'Signal_Line',\n",
    "    'Delta', 'Gain', 'Loss', 'Avg Gain', 'Avg Loss', 'RS', 'RSI',\n",
    "    'open_VIX', 'high_VIX', 'low_VIX', 'close_VIX', 'adj_close_VIX',\n",
    "    'open_USDX', 'high_USDX', 'low_USDX', 'close_USDX', 'Adj_USDX', 'Volume_USDX',\n",
    "    'open_AAPL', 'high_AAPL', 'low_AAPL', 'Adj_Close_APPL', 'Volume_AAPL',\n",
    "    'MACD_A', 'Signal_Line_A', 'H-L_A', 'ATR_A', 'TR_A',\n",
    "    'H-PC_A', 'L-PC_A', 'Delta_A', 'Gain_A', 'Loss_A',\n",
    "    'Avg Gain_A', 'Avg Loss_A', 'RS_A', 'RSI_A',\n",
    "    'EFFR', 'UNRATE', 'UMCSENT'\n",
    "]\n",
    "\n",
    "X = DF[features]\n",
    "y = DF['Close']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print RMSE (Root Mean Squared Error)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Decision plot for the first prediction in the test set\n",
    "shap.decision_plot(explainer.expected_value, shap_values.values[0], features=X_test.iloc[0], feature_names=X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0822c1-44dd-4851-92a0-807605326047",
   "metadata": {},
   "source": [
    "## LinearRegression shape value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371b735-22d6-4ac6-ac1b-ebc5ad45be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = DF[features]\n",
    "y = DF['Close']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print RMSE (Root Mean Squared Error)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "#shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "# Decision plot for the first prediction in the test set\n",
    "shap.decision_plot(explainer.expected_value, shap_values.values[0], features=X_test.iloc[0], feature_names=X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e4e92-9dd5-4ae8-b940-84070e1a8583",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor shape value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940294f-275e-4c00-84e0-9a3986df9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X = DF[features]\n",
    "y = DF['Close']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Decision Tree Regressor\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print RMSE (Root Mean Squared Error)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "# Decision plot for the first prediction in the test set\n",
    "shap.decision_plot(explainer.expected_value, shap_values.values[0], features=X_test.iloc[0], feature_names=X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae79b30-385d-4580-944d-2c3cc85a4f6d",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor shape value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc8ee2-79d7-4671-93c1-c674483148c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize JavaScript visualization in the notebook\n",
    "shap.initjs()\n",
    "\n",
    "X = DF[features]\n",
    "y = DF['Close']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print RMSE (Root Mean Squared Error)\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "# Decision plot for the first prediction in the test set\n",
    "shap.decision_plot(explainer.expected_value, shap_values.values[0], features=X_test.iloc[0], feature_names=X_test.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec011db-91c5-4b5f-94e7-febc4b26387f",
   "metadata": {},
   "source": [
    "# TRAIN TIME SERIES AND REGRESSION MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9847400-64b4-4e51-af42-b5218a050c75",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafccfa-55e4-43b4-a00a-4fb1c30ea598",
   "metadata": {},
   "source": [
    "\n",
    "## The models are fine-tuned by testing three learning rates (0.1, 0.01, 0.001), three batch sizes (4, 8, 16), six neuron configurations (10, 50, 100, 150, 200, 250) for single-layer LSTM, and three optimizers (Adam, Adagrad, Nadam). Each model is trained over 15 epochs. For multi-layer LSTM architectures, additional neuron configurations such as (10-50), (50-100), (100-150), (150-200), (10-50-100), and (20-70-120) are also tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b7ef4-830c-4dbf-9934-ae0e890b3ffd",
   "metadata": {},
   "source": [
    "##  Single layer LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff9432-4cdb-4df6-90e0-ba4816a3cf85",
   "metadata": {},
   "source": [
    "## Neuron =10, Optimizer= ADAM,  learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa84cf-2bd7-48db-8517-d249072901ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "\n",
    "features =DF[ [\n",
    "    \"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"\n",
    "]]\n",
    "\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(10, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6204a1-88ac-4bc5-bda5-2da525f439f4",
   "metadata": {},
   "source": [
    "## Neuron =50, Optimizer= ADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a9416-c284-4469-b40d-01a5b040baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Optionally save the results to a CSV file\n",
    "results_df.to_csv('learning_rate_batch_size_experiment_results50.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd64f93-a73e-4d98-ad68-0967c0ea47ea",
   "metadata": {},
   "source": [
    "## Neuron =100, Optimizer= ADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56dd0c0-4d61-4a2f-b16d-3276d234879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf37be-050f-47be-af7f-be422dc3202f",
   "metadata": {},
   "source": [
    "## Neuron = 150, Optimizer= ADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f963d0-3cf8-4548-9d16-6c0917647fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(150, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d092f2-4b5e-4373-af26-0af5c8685786",
   "metadata": {},
   "source": [
    "## Neuron = 200, Optimizer= ADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05084dd7-8083-4be9-9c76-26cfe7d2f417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model##############################################################################################3\n",
    "        model = Sequential([\n",
    "            LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b02b3-4456-4a9e-aaa7-061dff70dd3c",
   "metadata": {},
   "source": [
    "## Neuron = 250, Optimizer= ADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da64490-c9ac-43a7-97ba-fba094f10066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(250, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Optionally save the results to a CSV file\n",
    "results_df.to_csv('learning_rate_batch_size_experiment_results250.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6be60d-180e-46c0-a611-56a4c6ab6b7e",
   "metadata": {},
   "source": [
    "## Neuron = 10, Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac92c8-2797-4597-935a-46250a2ca031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(10, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a87bd-6f06-434f-87e7-9ad1055cdef7",
   "metadata": {},
   "source": [
    "## Neuron = 50, Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c04e67-5d17-4211-9af7-124f4b826789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca64985-f1aa-46b3-81d0-1bc75b3d08d2",
   "metadata": {},
   "source": [
    "## Neuron = 100, Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdcac69-624a-4750-89a0-8b4c798ea367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f66b85-1dd1-4ee5-adc2-7be02345926c",
   "metadata": {},
   "source": [
    "## Neuron = 150, Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8f9ab-f07b-41fe-845b-7114c2ae7542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(150, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2750d-f416-4480-bc27-790b385f9f99",
   "metadata": {},
   "source": [
    "## Neuron = 200, Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edbec8-83ab-40b0-accc-acfc3c68cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e76c04d-a5c2-4152-ace8-eb7987b7ad26",
   "metadata": {},
   "source": [
    "## Neuron = 250, Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114f534-affb-4142-ae55-6bb3795940a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(250, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5f9cb-3e4b-419f-ad02-9b5f07d57c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe249125-ed41-4e3e-b2f1-a703ad5674e6",
   "metadata": {},
   "source": [
    "## Neuron = 10, Optimizer= NADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5c278-c9bd-4cbb-a02d-24dfe393ebbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Nadam  # Import Nadam optimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(10, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Nadam(learning_rate=learning_rate)  # Use Nadam optimizer\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27809c6-b3c1-4d8d-8714-9257e0c82e17",
   "metadata": {},
   "source": [
    "## Neuron = 50, Optimizer= NADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9064fc-01a1-4a62-abe9-39ff5e22be35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Nadam(learning_rate=learning_rate)  # Use Nadam optimizer\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80cbcb-0bd1-4f4d-b07d-bbe8f08f2ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a9b4e54-0b98-4d71-894d-9a9d39fe287f",
   "metadata": {},
   "source": [
    "## Neuron = 100, Optimizer= NADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541525b-499b-488e-8253-13ee68c35907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Nadam(learning_rate=learning_rate)  # Use Nadam optimizer\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9dad7-9ba7-453d-becc-cb478c96dc60",
   "metadata": {},
   "source": [
    "# NADAM 150 N with 4 folds 3 learing rate and 3 batch size and avrage of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e6556-d6f7-45d0-b215-c341835bdaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(150, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Nadam(learning_rate=learning_rate)  # Use Nadam optimizer\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac28a4-97e6-4518-854f-17ee37b8a4d4",
   "metadata": {},
   "source": [
    "## Neuron = 200, Optimizer= NADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945721f-4600-4418-9927-da4f4162eb51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(200, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Nadam(learning_rate=learning_rate)  # Use Nadam optimizer\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc3fc6-4564-48cd-a147-248324d6c8f1",
   "metadata": {},
   "source": [
    "## Neuron = 250, Optimizer= NADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965625a0-6dc7-4b46-bf7d-ba43027d5144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the model\n",
    "        model = Sequential([\n",
    "            LSTM(250, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = Nadam(learning_rate=learning_rate)  # Use Nadam optimizer\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717dbf1-0726-418e-b687-9f9410cc1c6b",
   "metadata": {},
   "source": [
    "# Multi layer LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e2e53-59c2-490d-8f53-2eef9fdfe70a",
   "metadata": {},
   "source": [
    "## Neuron =(10-50), Optimizer= (ADAM,NADAM,Adagrad),  learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64413d48-90ec-4438-9d02-88a462302711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates, batch sizes, and optimizers to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "optimizers = {'Adam': Adam, 'Adagrad': Adagrad, 'Nadam': Nadam,}\n",
    "\n",
    "# Dictionaries to store metrics for each optimizer, learning rate, and batch size\n",
    "results = {\n",
    "    'optimizer': [],\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given optimizer, learning rate, and batch size\n",
    "def run_experiment(optimizer_name, optimizer_class, learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with optimizer {optimizer_name}, learning rate {learning_rate}, and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(50),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['optimizer'].append(optimizer_name)\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current optimizer, learning rate, and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each optimizer, learning rate, and batch size\n",
    "for optimizer_name, optimizer_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            run_experiment(optimizer_name, optimizer_class, lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb918a7-14ec-47a9-a998-a3d66b84bc06",
   "metadata": {},
   "source": [
    "## Neuron =(50-100), Optimizer= (ADAM,Adagrad,NADAM),  learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37018fa-d7cf-4593-9181-1e0bb81d4ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates, batch sizes, and optimizers to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "optimizers = {'Adam': Adam, 'Adagrad': Adagrad, 'Nadam': Nadam,}\n",
    "\n",
    "# Dictionaries to store metrics for each optimizer, learning rate, and batch size\n",
    "results = {\n",
    "    'optimizer': [],\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given optimizer, learning rate, and batch size\n",
    "def run_experiment(optimizer_name, optimizer_class, learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with optimizer {optimizer_name}, learning rate {learning_rate}, and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(100),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['optimizer'].append(optimizer_name)\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current optimizer, learning rate, and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each optimizer, learning rate, and batch size\n",
    "for optimizer_name, optimizer_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            run_experiment(optimizer_name, optimizer_class, lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bc88e-d549-4479-95c4-789ed0a5b01b",
   "metadata": {},
   "source": [
    "## Neuron = (100-150), Optimizer= (ADAM,Adagrad,NADAM), learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48879ac3-a3df-4116-964d-b21d991db6d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates, batch sizes, and optimizers to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "optimizers = {'Adam': Adam, 'Adagrad': Adagrad, 'Nadam': Nadam,}\n",
    "\n",
    "# Dictionaries to store metrics for each optimizer, learning rate, and batch size\n",
    "results = {\n",
    "    'optimizer': [],\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given optimizer, learning rate, and batch size\n",
    "def run_experiment(optimizer_name, optimizer_class, learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with optimizer {optimizer_name}, learning rate {learning_rate}, and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(150),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['optimizer'].append(optimizer_name)\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current optimizer, learning rate, and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each optimizer, learning rate, and batch size\n",
    "for optimizer_name, optimizer_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            run_experiment(optimizer_name, optimizer_class, lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d61b8-590a-427c-8440-dd3e43cd7ac8",
   "metadata": {},
   "source": [
    "## Neuron = (150-200), Optimizer= (ADAM,Adagrad,NADAM), learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2603c26-3def-4297-88fc-420e8df8dadb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates, batch sizes, and optimizers to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "optimizers = {'Adam': Adam, 'Adagrad': Adagrad, 'Nadam': Nadam,}\n",
    "\n",
    "# Dictionaries to store metrics for each optimizer, learning rate, and batch size\n",
    "results = {\n",
    "    'optimizer': [],\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given optimizer, learning rate, and batch size\n",
    "def run_experiment(optimizer_name, optimizer_class, learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with optimizer {optimizer_name}, learning rate {learning_rate}, and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(150, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(200),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        #plt.figure(figsize=(8, 6))\n",
    "        #plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        #plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        #plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        #plt.xlabel('Epochs')\n",
    "        #plt.ylabel('RMSE')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['optimizer'].append(optimizer_name)\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current optimizer, learning rate, and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each optimizer, learning rate, and batch size\n",
    "for optimizer_name, optimizer_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            run_experiment(optimizer_name, optimizer_class, lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd95cce-770a-4e97-8e2b-72224ef2a2db",
   "metadata": {},
   "source": [
    "## Neuron = (10-50-100), Optimizer= (Adagra,NADAM), learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b14d8-b6a0-4efa-9e1e-c3ff6df07a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates, batch sizes, and optimizers to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "optimizers = { 'Adagrad': Adagrad, 'Nadam': Nadam,}\n",
    "\n",
    "# Dictionaries to store metrics for each optimizer, learning rate, and batch size\n",
    "results = {\n",
    "    'optimizer': [],\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given optimizer, learning rate, and batch size\n",
    "def run_experiment(optimizer_name, optimizer_class, learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with optimizer {optimizer_name}, learning rate {learning_rate}, and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "        LSTM(10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        LSTM(100),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "\n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1)) \n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        #plt.figure(figsize=(8, 6))\n",
    "        #plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        #plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        #plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        #plt.xlabel('Epochs')\n",
    "        #plt.ylabel('RMSE')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['optimizer'].append(optimizer_name)\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current optimizer, learning rate, and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each optimizer, learning rate, and batch size\n",
    "for optimizer_name, optimizer_class in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            run_experiment(optimizer_name, optimizer_class, lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30986767-9565-4d85-807c-b4588fdab0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cede6ef4-b795-4488-b6e5-c72ef15fc47b",
   "metadata": {},
   "source": [
    "## Neuron = (10-50-100), Optimizer= (ADAM), learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef3611-644b-4fcb-907e-cf3d977b407d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "        LSTM(10, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        LSTM(100),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5eced-264e-498d-9911-b42e246ce639",
   "metadata": {},
   "source": [
    "## Neuron = (20-70-120), Optimizer= ADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3623c2-8a4c-4015-b598-c00417aec6ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "        LSTM(20, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(70, return_sequences=True),\n",
    "        LSTM(120),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "        #plt.figure(figsize=(6, 5))\n",
    "        #plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "        #plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "        #plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "        #plt.xlabel('Epochs')\n",
    "        #plt.ylabel('RMSE')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7795ad-fde7-4d36-a0f2-3f2262e8364d",
   "metadata": {},
   "source": [
    "## Neuron = (20-70-120), Optimizer= Adagrad, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e2f7d-2820-49f4-a81a-b4ae2def8313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(20, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(70, return_sequences=True),\n",
    "            LSTM(120),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "       # plt.figure(figsize=(8, 6))\n",
    "       # plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "       # plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "       # plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "       # plt.xlabel('Epochs')\n",
    "       # plt.ylabel('RMSE')\n",
    "       # plt.legend()\n",
    "       # plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54e924-dc63-4af5-a963-bf0b09534531",
   "metadata": {},
   "source": [
    "## Neuron = (20-70-120), Optimizer= NADAM, learning rates (0.1, 0.01, 0.001), batch sizes (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8817f9f-033e-4d54-8b47-f9bf7250b0db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Define learning rates and batch sizes to try\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [4, 8, 16]\n",
    "\n",
    "# Dictionaries to store metrics for each learning rate and batch size\n",
    "results = {\n",
    "    'learning_rate': [],\n",
    "    'batch_size': [],\n",
    "    'average_train_rmse': [],\n",
    "    'average_test_rmse': [],\n",
    "    'average_train_mape': [],\n",
    "    'average_test_mape': [],\n",
    "    'average_train_r': [],\n",
    "    'average_test_r': []\n",
    "}\n",
    "\n",
    "# Function to run training for a given learning rate and batch size\n",
    "def run_experiment(learning_rate, batch_size):\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "    train_rmse_values, test_rmse_values = [], []\n",
    "    train_mape_values, test_mape_values = [], []\n",
    "    train_r_values, test_r_values = [], []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(f\"Training fold {fold+1} with learning rate {learning_rate} and batch size {batch_size}\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define and compile the multi-layer LSTM model\n",
    "        model = Sequential([\n",
    "            LSTM(20, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(70, return_sequences=True),\n",
    "            LSTM(120),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = Nadam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "        # Set up the EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',   \n",
    "            patience=15,          \n",
    "            min_delta=0.001,      \n",
    "            restore_best_weights=True,  \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit the model\n",
    "        history = model.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test),\n",
    "                            verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluating metrics on the training and test sets\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "        train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "        train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "        test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "        test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "        train_rmse_values.append(train_rmse)\n",
    "        test_rmse_values.append(test_rmse)\n",
    "        train_mape_values.append(train_mape)\n",
    "        test_mape_values.append(test_mape)\n",
    "        train_r_values.append(train_r)\n",
    "        test_r_values.append(test_r)\n",
    "\n",
    "        print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "        print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "        print(f\"Train R-squared: {train_r:.3f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "        print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "        print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "        # Plotting the training and validation RMSE\n",
    "       # plt.figure(figsize=(8, 6))\n",
    "       # plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "       # plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "       # plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "       # plt.xlabel('Epochs')\n",
    "       # plt.ylabel('RMSE')\n",
    "       # plt.legend()\n",
    "       # plt.show()\n",
    "\n",
    "        # Reset Keras session to clear model weights\n",
    "        reset_keras()\n",
    "        fold += 1\n",
    "\n",
    "    # Calculating average metrics across folds\n",
    "    average_train_rmse = np.mean(train_rmse_values)\n",
    "    average_test_rmse = np.mean(test_rmse_values)\n",
    "    average_train_mape = np.mean(train_mape_values)\n",
    "    average_test_mape = np.mean(test_mape_values)\n",
    "    average_train_r = np.mean(train_r_values)\n",
    "    average_test_r = np.mean(test_r_values)\n",
    "\n",
    "    # Storing results\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['average_train_rmse'].append(average_train_rmse)\n",
    "    results['average_test_rmse'].append(average_test_rmse)\n",
    "    results['average_train_mape'].append(average_train_mape)\n",
    "    results['average_test_mape'].append(average_test_mape)\n",
    "    results['average_train_r'].append(average_train_r)\n",
    "    results['average_test_r'].append(average_test_r)\n",
    "\n",
    "    # Print average metrics for the current learning rate and batch size\n",
    "    print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "    print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "    print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "    print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "    print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "    print(f\"Average Test R-squared: {average_test_r:.3f}\")\n",
    "\n",
    "# Running experiments for each learning rate and batch size\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        run_experiment(lr, bs)\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b436a88-60d1-46a0-84bb-5ec1ceae3a89",
   "metadata": {},
   "source": [
    "# Best of LSTM Models: Adagrad Optimizer,  Learning Rate: 0.01, Batch Size: 4, Neuron Configuration: 200 \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7e5c7-2068-41ce-ba83-0083c6cd36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adagrad  # Using Adagrad optimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# Function to reset Keras session\n",
    "def reset_keras():\n",
    "    K.clear_session()\n",
    "    print(\"Keras session reset.\")\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    test_r = r2_score(y_true, y_pred)\n",
    "    return test_rmse, test_mape, test_r\n",
    "\n",
    "# Load data and prepare features (assuming DF is predefined DataFrame)\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "# Scaling features\n",
    "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_features = scaler_features.fit_transform(features)\n",
    "scaled_target = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Creating dataset\n",
    "time_steps = 10\n",
    "X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
    "\n",
    "# Setting up 5-fold cross-validation\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "train_rmse_values, test_rmse_values = [], []\n",
    "train_mape_values, test_mape_values = [], []\n",
    "train_r_values, test_r_values = [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(f\"Training fold {fold+1}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Define and compile the model\n",
    "    model = Sequential([\n",
    "        LSTM(20, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    optimizer = Adagrad(learning_rate=0.01)\n",
    "    model.compile(optimizer=optimizer, loss=rmse, metrics=[rmse])\n",
    "\n",
    "    # Set up the EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',   # Monitor the validation loss\n",
    "        patience=15,          # Number of epochs with no improvement after which training will be stopped\n",
    "        min_delta=0.001,      # Minimum change in the monitored quantity to qualify as an improvement\n",
    "        restore_best_weights=True,  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=15, batch_size=4, validation_data=(X_test, y_test),\n",
    "                        verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluating metrics on the training and test sets\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_pred_actual = scaler_target.inverse_transform(train_pred)\n",
    "    train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1))\n",
    "    train_rmse, train_mape, train_r = calculate_metrics(train_actual, train_pred_actual)\n",
    "\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_pred_actual = scaler_target.inverse_transform(test_pred)\n",
    "    test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "    test_rmse, test_mape, test_r = calculate_metrics(test_actual, test_pred_actual)\n",
    "\n",
    "    train_rmse_values.append(train_rmse)\n",
    "    test_rmse_values.append(test_rmse)\n",
    "    train_mape_values.append(train_mape)\n",
    "    test_mape_values.append(test_mape)\n",
    "    train_r_values.append(train_r)\n",
    "    test_r_values.append(test_r)\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "    print(f\"Train MAPE: {train_mape:.3f}%\")\n",
    "    print(f\"Train R-squared: {train_r:.3f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "    print(f\"Test MAPE: {test_mape:.3f}%\")\n",
    "    print(f\"Test R-squared: {test_r:.3f}\")\n",
    "\n",
    "    # Plotting the training and validation RMSE\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(history.history['rmse'], 'bo-', label='Training RMSE', color='darkblue')\n",
    "    plt.plot(history.history['val_rmse'], 'ro-', label='Validation RMSE', color='darkorange')\n",
    "    plt.title(f'Training and Validation RMSE - Fold {fold+1}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print specific predictions and actuals\n",
    "    for i in range(3):  # Print the first 3 predictions and actuals for each fold\n",
    "        print(f\"Fold {fold+1} Predicted: {test_pred_actual[i][0]:.2f}, Actual: {test_actual[i][0]:.2f}\")\n",
    "\n",
    "    # Plotting Actual vs Predicted Prices for the current fold\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(test_actual, label='Actual Prices', color='blue')\n",
    "    plt.plot(test_pred_actual, label='Predicted Prices', linestyle='--', color='red')\n",
    "    plt.title(f'Actual vs Predicted Prices - Fold {fold+1}')\n",
    "    plt.xlabel('Time (test set index)')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Reset Keras session to clear model weights\n",
    "    reset_keras()\n",
    "    fold += 1\n",
    "\n",
    "# Calculating average metrics across folds\n",
    "average_train_rmse = np.mean(train_rmse_values)\n",
    "average_test_rmse = np.mean(test_rmse_values)\n",
    "average_train_mape = np.mean(train_mape_values)\n",
    "average_test_mape = np.mean(test_mape_values)\n",
    "average_train_r = np.mean(train_r_values)\n",
    "average_test_r = np.mean(test_r_values)\n",
    "\n",
    "print(f\"Average Train RMSE: {average_train_rmse:.3f}\")\n",
    "print(f\"Average Test RMSE: {average_test_rmse:.3f}\")\n",
    "print(f\"Average Train MAPE: {average_train_mape:.3f}%\")\n",
    "print(f\"Average Test MAPE: {average_test_mape:.3f}%\")\n",
    "print(f\"Average Train R-squared: {average_train_r:.3f}\")\n",
    "print(f\"Average Test R-squared: {average_test_r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600f009-4346-4fa1-941b-60094f59f492",
   "metadata": {},
   "source": [
    "# Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c8a0a-4e9b-45ea-b45e-15256a2cfa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming DF is your DataFrame and 'Close' is the target variable\n",
    "\n",
    "# Prepare the data for Prophet\n",
    "data = DF.reset_index()  # Reset index to use the date column\n",
    "data = data.rename(columns={'index': 'ds', 'Close': 'y'})  # Rename columns as required by Prophet\n",
    "\n",
    "# Set cap and floor for the target variable if applicable\n",
    "data['cap'] = data['y'].max()  # Set the maximum value as cap\n",
    "data['floor'] = data['y'].min()  # Set the minimum value as floor\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "train_size = int(len(data) * 0.8)\n",
    "train = data[:train_size]\n",
    "test = data[train_size:]\n",
    "\n",
    "# Initialize and fit the Prophet model with custom parameters\n",
    "model = Prophet(\n",
    "    changepoint_prior_scale=0.05,  # Adjust for more/less flexibility in the trend\n",
    "    seasonality_prior_scale=10.0,  # Adjust for more/less flexibility in seasonal patterns\n",
    "    yearly_seasonality=True,       # Enable yearly seasonality\n",
    "    weekly_seasonality=True,       # Enable weekly seasonality\n",
    "    daily_seasonality=False        # Disable daily seasonality (adjust if necessary)\n",
    ")\n",
    "\n",
    "# Add custom quarterly seasonality\n",
    "model.add_seasonality(name='quarterly', period=91.25, fourier_order=8)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(train)\n",
    "\n",
    "# Make predictions on the entire dataset (train + test)\n",
    "future = model.make_future_dataframe(periods=len(test))\n",
    "future['cap'] = data['cap']\n",
    "future['floor'] = data['floor']\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Extract the forecasted values for the training set\n",
    "train_predictions = forecast[['ds', 'yhat']].iloc[:train_size].set_index('ds')\n",
    "train_actual = train.set_index('ds')\n",
    "\n",
    "# Extract the forecasted values for the test set\n",
    "test_predictions = forecast[['ds', 'yhat']].iloc[train_size:].set_index('ds')\n",
    "test_actual = test.set_index('ds')\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred):\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "train_mse = mean_squared_error(train_actual['y'], train_predictions['yhat'])\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train_actual['y'], train_predictions['yhat'])\n",
    "train_mpa = mean_percentage_accuracy(train_actual['y'], train_predictions['yhat'])\n",
    "train_r2 = r2_score(train_actual['y'], train_predictions['yhat'])\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "test_mse = mean_squared_error(test_actual['y'], test_predictions['yhat'])\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_actual['y'], test_predictions['yhat'])\n",
    "test_mpa = mean_percentage_accuracy(test_actual['y'], test_predictions['yhat'])\n",
    "test_r2 = r2_score(test_actual['y'], test_predictions['yhat'])\n",
    "\n",
    "# Print metrics for the training set\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Print metrics for the test set\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n",
    "\n",
    "# Plot the forecast\n",
    "model.plot(forecast)\n",
    "plt.show()\n",
    "\n",
    "# Plot components of the forecast\n",
    "model.plot_components(forecast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c9ae1-7d6e-44f2-a51c-39c6899be5f3",
   "metadata": {},
   "source": [
    "# Hyper tuning Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388f406-9283-44b3-992d-4fea8596f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred):\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Prepare the data for Prophet\n",
    "data = DF.reset_index()  # Reset index to use the date column\n",
    "data = data.rename(columns={'index': 'ds', 'Close': 'y'})  # Rename columns as required by Prophet\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data = data.iloc[:-20]\n",
    "test_data = data.iloc[-20:]\n",
    "\n",
    "# Define a wider range of hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1.0],\n",
    "    'holidays_prior_scale': [0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the best MSE to a large number\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Grid search for hyperparameters\n",
    "for cps in param_grid['changepoint_prior_scale']:\n",
    "    for sps in param_grid['seasonality_prior_scale']:\n",
    "        for hps in param_grid['holidays_prior_scale']:\n",
    "            model = Prophet(changepoint_prior_scale=cps, seasonality_prior_scale=sps, holidays_prior_scale=hps)\n",
    "            model.fit(train_data)\n",
    "            forecast = model.predict(train_data)\n",
    "            mse = mean_squared_error(train_data['y'], forecast['yhat'])\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_params = {'changepoint_prior_scale': cps, 'seasonality_prior_scale': sps, 'holidays_prior_scale': hps}\n",
    "\n",
    "print(\"Best parameters found:\", best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.fit(train_data)\n",
    "\n",
    "# Make predictions for the entire dataset including future 20 days\n",
    "future = model.make_future_dataframe(periods=20)  # Extend the dataframe by 20 days\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Extract the next 20 days' predictions\n",
    "next_20_days_predictions = forecast[['ds', 'yhat']].tail(20)\n",
    "\n",
    "# Print the next 20 days' predictions\n",
    "print(\"Next 20 Days' Predictions:\")\n",
    "print(next_20_days_predictions)\n",
    "\n",
    "# Calculate errors on the training set\n",
    "train_pred = model.predict(train_data)\n",
    "train_mse = mean_squared_error(train_data['y'], train_pred['yhat'])\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train_data['y'], train_pred['yhat'])\n",
    "train_r2 = r2_score(train_data['y'], train_pred['yhat'])\n",
    "train_mpa = mean_percentage_accuracy(train_data['y'], train_pred['yhat'])\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate errors on the test set\n",
    "test_pred = forecast[['ds', 'yhat']].iloc[-40:-20]  # Get the predicted values corresponding to the test set\n",
    "test_mse = mean_squared_error(test_data['y'], test_pred['yhat'])\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_data['y'], test_pred['yhat'])\n",
    "test_r2 = r2_score(test_data['y'], test_pred['yhat'])\n",
    "test_mpa = mean_percentage_accuracy(test_data['y'], test_pred['yhat'])\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n",
    "\n",
    "# Plot the actual and forecasted values including the next 20 days' predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data['ds'], data['y'], label='Actual Close Price', color='blue')\n",
    "plt.plot(forecast['ds'], forecast['yhat'], label='Predicted Close Price', color='orange')\n",
    "plt.axvline(x=next_20_days_predictions['ds'].iloc[0], color='red', linestyle='--', label='Start of Prediction')\n",
    "plt.scatter(next_20_days_predictions['ds'], next_20_days_predictions['yhat'], color='red')\n",
    "plt.title('Actual vs Predicted Close Prices with Next 20 Days Predictions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4e631-6bb2-482a-9c80-74a71d18415c",
   "metadata": {},
   "source": [
    "# LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40897f04-19f4-43d3-8b4d-d22604924a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred):\n",
    "    return np.mean(100 - np.abs((y_true - y_pred) / y_true) * 100)\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with all the features and the target 'Close'\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258932c-d692-4cc6-b52c-20fb6f0a7f01",
   "metadata": {},
   "source": [
    "## Hyper Tuning linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08815cba-69b1-4f56-9284-af4c30beac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred):\n",
    "    return np.mean(100 - np.abs((y_true - y_pred) / y_true) * 100)\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with all the features and the target 'Close'\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features\n",
    "y = target\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1.0, 10.0, 100.0, 200.0, 300.0]  # Regularization strength\n",
    "}\n",
    "\n",
    "# Initialize the Ridge Regression model\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score (negative MSE): \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the training set with the best model\n",
    "y_train_pred = best_ridge_model.predict(X_train)\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_test_pred = best_ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate and print training metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\nTraining Metrics with the best model:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate and print test metrics\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTesting Metrics with the best model:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe56e1-f498-4d62-b2cc-21ac1689e533",
   "metadata": {},
   "source": [
    "# RandomForestRegressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb774b9-e64b-4de8-a17f-f12b6aec47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf931c0-81d9-415b-9a48-0f8873c8594a",
   "metadata": {},
   "source": [
    "## Hyper Tuning RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987857b4-49ed-42fd-8ce3-20c84656d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search over (simplified for RandomizedSearchCV)\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_distributions, n_iter=10, cv=3,\n",
    "                                   scoring='neg_mean_squared_error', n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit the model with RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score (negative MSE): \", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Predict on the training set with the best model\n",
    "y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate and print training metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"\\nTraining Metrics with the best model:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate and print test metrics\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTesting Metrics with the best model:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131821fc-916c-42e7-b41b-4eb5a8404d83",
   "metadata": {},
   "source": [
    "# KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c969fd-d207-485b-a452-53d98ed837f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets (75% train, 25% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize the KNeighborsRegressor model\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = knn_model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics for both training and test sets\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R): {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3b06d-6b14-4d23-93e8-4a61f803da66",
   "metadata": {},
   "source": [
    "## Hyper Tuning KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb731e-a89c-4c96-bb23-00ebd0086e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Initialize the KNeighborsRegressor model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Define the scorer\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring=scorer, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score (negative MSE): \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_test_pred = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate and print test metrics\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTesting Metrics with the best model:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45c4f4-06f2-4853-8d60-5679f0a59ddd",
   "metadata": {},
   "source": [
    "## DecisionTree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f8f05-6d71-4d7f-b803-f59ee9269c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the DecisionTreeRegressor model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_train_pred = dt_model.predict(X_train)\n",
    "y_test_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ad9be-eb4a-46dc-9987-f1ad7e09724b",
   "metadata": {},
   "source": [
    "## Hyper Tuning DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a8491-7c24-4d4b-a9dd-d048b6f3abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the DecisionTreeRegressor model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score (neg MSE): {grid_search.best_score_}\")\n",
    "\n",
    "# Predict on the training and test sets using the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d9cda-911c-45dc-b548-a066d188e439",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa190d3e-ee54-4a78-905b-fec3ef2e5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "                                     max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_train_pred = gb_model.predict(X_train)\n",
    "y_test_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print training and test metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"MSE: {train_mse}, RMSE: {train_rmse}, MAE: {train_mae}, MPA: {train_mpa}%, R²: {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"MSE: {test_mse}, RMSE: {test_rmse}, MAE: {test_mae}, MPA: {test_mpa}%, R²: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36b8e9-6bda-40b8-a5d1-a447d7dc0ef5",
   "metadata": {},
   "source": [
    "## Hyper Tuning GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03fb7d-8739-4efa-a695-1a60c9d5d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV object with GradientBoostingRegressor and negative mean squared error scoring\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE as scoring metric\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the full training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best MSE (negative) score from cross-validation\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validated MSE: \", -grid_search.best_score_)\n",
    "\n",
    "# Best model according to grid search\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the training and test sets using the best model\n",
    "y_train_pred = best_gb_model.predict(X_train)\n",
    "y_test_pred = best_gb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print training and test metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"MSE: {train_mse}, RMSE: {train_rmse}, MAE: {train_mae}, MPA: {train_mpa}%, R²: {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"MSE: {test_mse}, RMSE: {test_rmse}, MAE: {test_mae}, MPA: {test_mpa}%, R²: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c49fb-e697-4142-81d5-222b35e2557d",
   "metadata": {},
   "source": [
    "## xgboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986ba44-6a87-40b5-8b4f-9ed79209966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBRegressor model with chosen parameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training and test sets\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print training and test metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"MSE: {train_mse}, RMSE: {train_rmse}, MAE: {train_mae}, MPA: {train_mpa}%, R²: {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"MSE: {test_mse}, RMSE: {test_rmse}, MAE: {test_mae}, MPA: {test_mpa}%, R²: {test_r2}\")\n",
    "\n",
    "# Plotting Actual vs Predicted values for training and test sets\n",
    "def plot_actual_vs_predicted(ax, actual, predicted, title, color):\n",
    "    ax.scatter(actual, predicted, alpha=0.5, color=color, edgecolor='k')\n",
    "    ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2, label='Ideal Fit')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Actual Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.legend()\n",
    "\n",
    "# Create figure and axes for the subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Plot for the training set\n",
    "plot_actual_vs_predicted(axes[0], y_train, y_train_pred, 'Training Set - Actual vs. Predicted', 'blue')\n",
    "\n",
    "# Plot for the test set\n",
    "plot_actual_vs_predicted(axes[1], y_test, y_test_pred, 'Test Set - Actual vs. Predicted', 'red')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00b306-c988-49ee-8672-c648a80445f6",
   "metadata": {},
   "source": [
    "## Hyper Tuning xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395da52-2c03-433d-b28e-a159d62b8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Function to calculate Mean Percentage Accuracy (MPA)\n",
    "def mean_percentage_accuracy(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(100 - (np.abs((y_true - y_pred) / y_true) * 100))\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'DF' is your DataFrame with the provided features and target\n",
    "features = DF[[\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]]\n",
    "target = DF['Close']\n",
    "\n",
    "X = features  # Use the selected features\n",
    "y = target  # Target variable\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBRegressor model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV object with XGBRegressor and negative mean squared error scoring\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE as scoring metric\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the full training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best MSE (negative) score from cross-validation\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validated MSE: \", -grid_search.best_score_)\n",
    "\n",
    "# Best model according to grid search\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the training and test sets using the best model\n",
    "y_train_pred = best_xgb_model.predict(X_train)\n",
    "y_test_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mpa = mean_percentage_accuracy(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mpa = mean_percentage_accuracy(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print training and test metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"MSE: {train_mse}, RMSE: {train_rmse}, MAE: {train_mae}, MPA: {train_mpa}%, R²: {train_r2}\")\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"MSE: {test_mse}, RMSE: {test_rmse}, MAE: {test_mae}, MPA: {test_mpa}%, R²: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd443d86-8fcb-42b6-9e46-51c3fdcebd48",
   "metadata": {},
   "source": [
    "# SARIMAX (Seasonal ARIMA with Exogenous Regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104aae62-f23e-4746-b3f7-07dd38d37b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "exogenous_features = [\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]\n",
    "target = 'Close'\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "DF = DF.sort_index()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(DF) * 0.7)\n",
    "train, test = DF.iloc[:train_size], DF.iloc[train_size:]\n",
    "\n",
    "# Extract exogenous variables\n",
    "train_exog = train[exogenous_features]\n",
    "test_exog = test[exogenous_features]\n",
    "\n",
    "# Initialize the SARIMAX model\n",
    "model = SARIMAX(train[target], \n",
    "                exog=train_exog, \n",
    "                order=(1, 1, 1), \n",
    "                seasonal_order=(1, 1, 1, 12))\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model_fit.predict(start=0, end=len(train)-1, exog=train_exog)\n",
    "test_predictions = model_fit.predict(start=len(train), end=len(DF)-1, exog=test_exog)\n",
    "\n",
    "# Calculate errors on the training set\n",
    "train_mse = mean_squared_error(train[target], train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train[target], train_predictions)\n",
    "train_r2 = r2_score(train[target], train_predictions)\n",
    "train_mpa = np.mean(100 - (np.abs((train[target] - train_predictions) / train[target]) * 100))\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate errors on the test set\n",
    "test_mse = mean_squared_error(test[target], test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test[target], test_predictions)\n",
    "test_r2 = r2_score(test[target], test_predictions)\n",
    "test_mpa = np.mean(100 - (np.abs((test[target] - test_predictions) / test[target]) * 100))\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff07509-8aa0-4efe-9d97-3cc1d5865bf7",
   "metadata": {},
   "source": [
    "# SARIMAX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eef67a-545c-432a-b505-79a507673810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data\n",
    "exogenous_features = [\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "                      \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\",\n",
    "                      \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\",\n",
    "                      \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\",\n",
    "                      \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\",\n",
    "                      \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\",\n",
    "                      \"Price_Change\", \"RSI_A\"]\n",
    "target = 'Close'\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "DF = DF.sort_index()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(DF) * 0.7)\n",
    "train, test = DF.iloc[:train_size], DF.iloc[train_size:]\n",
    "\n",
    "# Extract exogenous variables\n",
    "train_exog = train[exogenous_features]\n",
    "test_exog = test[exogenous_features]\n",
    "\n",
    "# Initialize the SARIMAX model\n",
    "model = SARIMAX(train[target], \n",
    "                exog=train_exog, \n",
    "                order=(1, 1, 1), \n",
    "                seasonal_order=(1, 1, 1, 12))\n",
    "\n",
    "# Fit the model\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# Make predictions on the train and test sets\n",
    "train_predictions = model_fit.predict(start=0, end=len(train)-1, exog=train_exog)\n",
    "test_predictions = model_fit.predict(start=len(train), end=len(DF)-1, exog=test_exog)\n",
    "\n",
    "# Predict the next 15 days\n",
    "future_exog = test_exog[-15:]  # Assuming you have exogenous data for the next 15 days\n",
    "future_predictions = model_fit.forecast(steps=15, exog=future_exog)\n",
    "\n",
    "# Print the predicted values for the next 15 days\n",
    "print(\"\\nPredicted Close Prices for the next 15 days:\")\n",
    "print(future_predictions)\n",
    "\n",
    "# Calculate errors on the training set\n",
    "train_mse = mean_squared_error(train[target], train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train[target], train_predictions)\n",
    "train_r2 = r2_score(train[target], train_predictions)\n",
    "train_mpa = np.mean(100 - (np.abs((train[target] - train_predictions) / train[target]) * 100))\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate errors on the test set\n",
    "test_mse = mean_squared_error(test[target], test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test[target], test_predictions)\n",
    "test_r2 = r2_score(test[target], test_predictions)\n",
    "test_mpa = np.mean(100 - (np.abs((test[target] - test_predictions) / test[target]) * 100))\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663d9ba-3c0a-4a6a-9681-46a307ad02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the actual vs. predicted values for the next 15 days\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot for testing set (last 15 days for comparison)\n",
    "plt.plot(test.index[-15:], test[target][-15:], label='Actual Close (Last 15 Days of Test)', color='blue')\n",
    "\n",
    "# Plot for future predictions (next 15 days)\n",
    "plt.plot(test.index[-15:], future_predictions, label='Predicted Close (Next 15 Days)', color='green')\n",
    "\n",
    "plt.title('Predicted Close Prices for the Next 15 Days')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455ad97-ed2c-4c8f-b772-dadf4b1a289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the close series\n",
    "close_series = DF['Close']\n",
    "\n",
    "# Check for stationarity and apply differencing if necessary\n",
    "def test_stationarity(timeseries):\n",
    "    rolmean = timeseries.rolling(window=12).mean()\n",
    "    rolstd = timeseries.rolling(window=12).std()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(timeseries, color='blue', label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "# Test stationarity of the original close series\n",
    "test_stationarity(close_series)\n",
    "\n",
    "# Apply differencing if needed\n",
    "close_series_diff = close_series.diff().dropna()\n",
    "test_stationarity(close_series_diff)\n",
    "\n",
    "# Prepare the data\n",
    "exogenous_features = [\"Adj Close\", \"AVG_price\", \"Low\", \"High\", \"Open\", \"low_AAPL\", \"close_AAPL\",\n",
    "    \"open_AAPL\", \"high_AAPL\", \"Adj_Close_APPL\", \"ATR_A\", \"Avg Gain_A\", \"H-L_A\", \n",
    "    \"TR_A\", \"Avg Gain\", \"H-PC_A\", \"ATR\", \"L-PC_A\", \"TR\", \"H-L\", \"H-PC\", \"Gain_A\", \n",
    "    \"low_USDX\", \"Adj_USDX\", \"close_USDX\", \"open_USDX\", \"high_USDX\", \"L-PC\", \n",
    "    \"Volume_USDX\", \"Volume\", \"Gain\", \"Signal_Line\", \"MACD\", \"Signal_Line_A\", \n",
    "    \"MACD_A\", \"EFFR\", \"Delta\", \"Price_AVG_Change\", \"RS_A\", \"Delta_A\", \n",
    "    \"Price_Change\", \"RSI_A\"]\n",
    "\n",
    "target = 'Close'\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "DF = DF.sort_index()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(DF) * 0.7)\n",
    "train, test = DF.iloc[:train_size], DF.iloc[train_size:]\n",
    "\n",
    "# Scale the exogenous features\n",
    "scaler = StandardScaler()\n",
    "train_exog = scaler.fit_transform(train[exogenous_features])\n",
    "test_exog = scaler.transform(test[exogenous_features])\n",
    "\n",
    "# Define a function to evaluate the model\n",
    "def evaluate_sarimax_model(train, test, train_exog, test_exog, order, seasonal_order):\n",
    "    model = SARIMAX(train, exog=train_exog, order=order, seasonal_order=seasonal_order)\n",
    "    model_fit = model.fit(disp=False)\n",
    "    \n",
    "    train_pred = model_fit.predict(start=0, end=len(train)-1, exog=train_exog)\n",
    "    test_pred = model_fit.predict(start=len(train), end=len(DF)-1, exog=test_exog)\n",
    "    \n",
    "    train_mse = mean_squared_error(train, train_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_mae = mean_absolute_error(train, train_pred)\n",
    "    train_r2 = r2_score(train, train_pred)\n",
    "    \n",
    "    test_mse = mean_squared_error(test, test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(test, test_pred)\n",
    "    test_r2 = r2_score(test, test_pred)\n",
    "    \n",
    "    return train_rmse, train_mae, train_r2, test_rmse, test_mae, test_r2\n",
    "\n",
    "# Fit the SARIMAX model (using example parameters)\n",
    "order = (3, 1, 3)  # Replace with your specific parameters\n",
    "seasonal_order = (1, 1, 1, 12)  # Replace with your specific parameters\n",
    "model = SARIMAX(train[target], exog=train_exog, order=order, seasonal_order=seasonal_order)\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model_fit.predict(start=0, end=len(train)-1, exog=train_exog)\n",
    "test_predictions = model_fit.predict(start=len(train), end=len(DF)-1, exog=test_exog)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(train[target], train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train[target], train_predictions)\n",
    "train_r2 = r2_score(train[target], train_predictions)\n",
    "train_mpa = np.mean(100 - (np.abs((train[target] - train_predictions) / train[target]) * 100))\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(test[target], test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test[target], test_predictions)\n",
    "test_r2 = r2_score(test[target], test_predictions)\n",
    "test_mpa = np.mean(100 - (np.abs((test[target] - test_predictions) / test[target]) * 100))\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(train.index, train[target], label='Train Actual')\n",
    "plt.plot(train.index, train_predictions, label='Train Predicted')\n",
    "plt.plot(test.index, test[target], label='Test Actual')\n",
    "plt.plot(test.index, test_predictions, label='Test Predicted')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Close Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c7298-fce6-43d9-b76c-cdf8c0011414",
   "metadata": {},
   "source": [
    "# Arima Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e06d383-ca55-4c00-87dd-eb87bdc2fd21",
   "metadata": {},
   "source": [
    "#  Fit the ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c0f9a-8cde-4fc4-b3f1-73f51a10b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming 'data' is a DataFrame and you're interested in a specific column (e.g., 'Close')\n",
    "data = DF['Close']\n",
    "\n",
    "# Convert the series to numeric, and handle any missing values\n",
    "data = pd.to_numeric(data, errors='coerce').dropna()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "# Fit the ARIMA model on the training set\n",
    "model = ARIMA(train, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary of the model\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model_fit.predict(start=0, end=len(train)-1)\n",
    "test_predictions = model_fit.forecast(steps=len(test))\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_mse = mean_squared_error(train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train, train_predictions)\n",
    "train_mape = mean_absolute_percentage_error(train, train_predictions)\n",
    "train_r2 = r2_score(train, train_predictions)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mse = mean_squared_error(test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test, test_predictions)\n",
    "test_mape = mean_absolute_percentage_error(test, test_predictions)\n",
    "test_r2 = r2_score(test, test_predictions)\n",
    "\n",
    "# Print metrics for the training set\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {train_mape}%\")\n",
    "print(f\"R-squared (R²): {train_r2}\")\n",
    "\n",
    "# Print metrics for the test set\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {test_mape}%\")\n",
    "print(f\"R-squared (R²): {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef0d2a-004f-4699-9b89-3e1c90f2e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load your data\n",
    "# Assuming DF is already loaded and 'Close' is the target variable\n",
    "data = DF['Close']\n",
    "\n",
    "# Function to evaluate an ARIMA model\n",
    "def evaluate_arima_model(train, test, arima_order):\n",
    "    model = ARIMA(train, order=arima_order)\n",
    "    model_fit = model.fit()\n",
    "    train_pred = model_fit.predict(start=0, end=len(train)-1, dynamic=False)\n",
    "    test_pred = model_fit.predict(start=len(train), end=len(data)-1, dynamic=False)\n",
    "    \n",
    "    train_mse = mean_squared_error(train, train_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_mae = mean_absolute_error(train, train_pred)\n",
    "    train_r2 = r2_score(train, train_pred)\n",
    "    \n",
    "    test_mse = mean_squared_error(test, test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(test, test_pred)\n",
    "    test_r2 = r2_score(test, test_pred)\n",
    "    \n",
    "    return train_rmse, train_mae, train_r2, test_rmse, test_mae, test_r2\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "# Define the p, d, q parameters to take any value between 0 and 3\n",
    "p = d = q = range(0, 4)\n",
    "\n",
    "# Generate all different combinations of p, d and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Perform grid search to find the best ARIMA parameters\n",
    "best_score, best_params = float(\"inf\"), None\n",
    "for param in pdq:\n",
    "    try:\n",
    "        train_rmse, train_mae, train_r2, test_rmse, test_mae, test_r2 = evaluate_arima_model(train, test, param)\n",
    "        if test_rmse < best_score:\n",
    "            best_score, best_params = test_rmse, param\n",
    "            print(f'ARIMA{param} - Test RMSE: {test_rmse:.4f}')\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f'Best ARIMA{best_params} - Test RMSE: {best_score:.4f}')\n",
    "\n",
    "# Fit the best ARIMA model\n",
    "best_model = ARIMA(train, order=best_params)\n",
    "best_model_fit = best_model.fit()\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = best_model_fit.predict(start=0, end=len(train)-1, dynamic=False)\n",
    "test_predictions = best_model_fit.predict(start=len(train), end=len(data)-1, dynamic=False)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_mse = mean_squared_error(train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train, train_predictions)\n",
    "train_r2 = r2_score(train, train_predictions)\n",
    "train_mpa = np.mean(100 - (np.abs((train - train_predictions) / train) * 100))\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {train_mpa}%\")\n",
    "print(f\"R-squared (R2): {train_r2}\")\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_mse = mean_squared_error(test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test, test_predictions)\n",
    "test_r2 = r2_score(test, test_predictions)\n",
    "test_mpa = np.mean(100 - (np.abs((test - test_predictions) / test) * 100))\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae}\")\n",
    "print(f\"Mean Percentage Accuracy (MPA): {test_mpa}%\")\n",
    "print(f\"R-squared (R2): {test_r2}\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(train.index, train, label='Train Actual')\n",
    "plt.plot(train.index, train_predictions, label='Train Predicted')\n",
    "plt.plot(test.index, test, label='Test Actual')\n",
    "plt.plot(test.index, test_predictions, label='Test Predicted')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Close Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409f124-87fc-46c1-825d-7737438ca3d0",
   "metadata": {},
   "source": [
    "# Conclusion and findings\n",
    "This project involved an in-depth analysis of stock market behavior, particularly focusing on the trends and patterns in the S&P 500's closing prices over a 24-year period. By examining the movement behavior of the stock market, the project aimed to identify the factors driving price fluctuations and to use these insights as a foundation for developing predictive models. A significant aspect of the project was the optimization of Long Short-Term Memory (LSTM) models, where various hyperparameters such as learning rates, batch sizes, and neuron configurations were fine-tuned to enhance prediction accuracy. Additionally, the project compared LSTM models with other time series and regression models, including ARIMA, SARIMAX, Prophet and Regression models to determine the most accurate model for forecasting the S&P 500’s closing prices. The goals of the project were to analyze the S&P 500’s price behavior, optimize LSTM models for better predictive performance, and identify the most reliable model for accurate stock market forecasting. Ultimately, this project aimed to advance the understanding of stock market dynamics and improve the tools available for financial forecasting, benefiting investors, analysts, and policymakers in making informed decisions.\n",
    "\n",
    "### Main findings\r\n",
    "Tuning LSTM Models: The primary goal was to enhance the performance of LSTM models through meticulous hyperparameter tuning. Different configurations of learning rates, batch sizes, and neuron counts were tested, with the Adagrad-optimized single-layer LSTM model emerging as the most accurate. This model, characterized by a learning rate of 0.01, a batch size of 4, and 200 neurons, demonstrated superior RMSE, MAPE, and R² values, making it the most reliable for predicting the S&P 500’s closing prices.\r\n",
    "Comparison of Regression and Time Series Models: The second goal was to evaluate the effectiveness of various regression and time series models, including ARIMA, SARIMA, Prophet, Linear Regression, Random Forest, and XGBoost. The analysis demonstrated that traditional time series models like SARIMAX provided valuable insights into long-term trends, while ensemble methods like Random Forest excelled at capturing complex patterns. Among the regression models, Linear Regression emerged as the best performer, offering a strong balance of simplicity and accuracy. However, when combining the strengths of these approaches, the tuned LSTM model consistently outperformed all others, making it the most reliable choice for predicting stock prices.\r\n",
    "Analysis of Stock Market Behavior: Lastly, The project involved analyzing 24 years of stock market data to uncover the key factors influencing market behavior. Significant events like the 2008 Financial Crisis and the COVID-19 pandemic were highlighted as major disruptions, causing sharp declines in prices and spikes in trading volume. The analysis also identified the unemployment rate, consumer sentiment, the U.S. dollar index (USDX), and the Volatility Index (VIX) as crucial factors affecting market movements. Additionally, the tech sector, especially Apple Inc. (AAPL), played a significant role in overall market performance. Despite these disruptions, the market demonstrated a consistent long-term upward trend, driven by strong economic fundamentals and investor confidence.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f19cb-ea0d-42b2-af0b-51b784c0f8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
